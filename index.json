[{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, I will introduce my Worklog. It details how I approached and completed the tasks, the specific duration of the internship program, and a summary of what I accomplished and learned throughout this period :\nWeek 1: Getting familiar with Cloud Computing, IAM, and Cost Management (AWS Budgets)\nWeek 2: Building Network Infrastructure: Multi-AZ VPC, Security, and Load Balancing\nWeek 3: Implementing Hybrid DNS Architecture with Route 53 Resolver\nWeek 4: Getting started with AWS Console/CLI, EC2, and Basic Storage (S3, Storage Gateway)\nWeek 5: Advanced Storage (S3 Glacier/Snow Family), VM Import, and Windows File Server (FSx)\nWeek 6: Advanced FSx Administration, Content Delivery (CloudFront), and IAM Fundamentals\nWeek 7: Enterprise Security Governance (Organizations, Security Hub) and Automation with Lambda\nWeek 8: Advanced Identity Management (IAM), Encryption (KMS), and System Auditing (CloudTrail/Athena)\nWeek 9: IAM Security Optimization and Web Application Deployment with Relational Database (RDS)\nWeek 10: Database Migration (DMS/SCT) and Building Serverless Data Analytics Pipelines\nWeek 11: NoSQL Databases (DynamoDB), Cost Management, and Data Preparation with Glue DataBrew\nWeek 12: Big Data Analytics (EMR, Redshift), Visualization (QuickSight), and Course Wrap-up\n"},{"uri":"https://datngo196.github.io/Internship_Report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://datngo196.github.io/Internship_Report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Ngo Huu Dat\nPhone Number: 0911449689\nEmail: datngo2005@gmail.com\nUniversity: FPT University\nMajor: Artificial Intelligence\nClass:\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Master the core concepts of Cloud Computing as defined by AWS Understand the AWS Global Infrastructure, including Regions, Availability Zones (AZs), and Edge Locations Familiarize with fundamental management tools (Console, IAM) and initial cost optimization methods on AWS Complete initial account setup, implementing security (MFA, IAM User), and budget management (AWS Budget) Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Research Cloud definition, payment models, core benefits (cost optimization, elasticity, global scaling). Research AWS Global Infrastructure (Data Center, AZ, Region, Edge Locations) 09/01/2025 09/01/2025 3 - Hands-on practice creating an AWS account (including email/phone/payment verification). Set up a Virtual MFA Device for the Root User for security 09/02/2025 09/02/2025 4 - Study AWS management tools (Console, Root/IAM User, CLI, SDK). Practice creating an IAM Admin Group and an IAM Admin User 09/03/2025 09/03/2025 5 - Research discounted payment options (On-Demand, RI, Saving Plans, Spot Instances) and the Serverless model. Practice creating an AWS Budget using a Template (monthly) 09/04/2025 09/04/2025 6 - Practice creating a Custom Cost Budget and a Usage Budget (e.g., limiting EC2 hours). Research AWS Support packages (Basic, Developer, Business, Enterprise). Clean up resources (Clean Up Budgets) 09/05/2025 09/05/2025 Week 1 Achievements: Understood the Cloud definition: On-demand delivery of IT resources over the Internet with pay-as-you-go pricing. Mastered core benefits, including cost optimization and the ability to elastically scale resources. Understood Global Infrastructure: Availability Zones (AZs) are key for fault isolation, and deploying across a minimum of 2 AZs is recommended for high availability. Completed account creation and security setup: MFA was successfully configured for the Root User. Created the IAM Admin Group and User, adhering to the security best practice of restricting Root User access. Familiarized with cost models: On-Demand (highest cost), Long-term commitment (RI/Saving Plans), and Temporary/Spare capacity (Spot Instances, up to 90% discount). Successfully set up AWS Budgets to monitor costs (Cost Budget) and track resource consumption limits (Usage Budget). Differentiated the 4 main AWS Support tiers (Basic free tier, Developer, Business, Enterprise) "},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.1-event1/","title":"AWS First Cloud Journey Community Day","tags":[],"description":"","content":"Summary Report: “AWS First Cloud Journey Community Day” Event Objectives Foster the AWS community vision and connect industry professionals. Demonstrate practical applications of Generative AI in Enterprise, Banking, and Academic sectors. Explore advanced architectures including RAG (Retrieval-Augmented Generation) and Multi-Agent Systems. Showcase serverless implementations for scalable AI solutions. Speakers Nguyễn Gia Hưng – Head of Solutions Architect, AWS Phạm Tiến Thuận Phát, Lê Minh Nghĩa, Trần Đoàn Công Lý – Enterprise Software Đinh Lê Hoàng Anh, Nguyễn Tài Minh Huy – Academic Sector Kiệt Lâm, Nguyễn Ngọc Quỳnh Mai – Banking / Internal IT Lê Phạm Ngọc Uyển, Phan Thị Thanh Thảo, Hồ Điền Đăng Khoa, Nguyễn Quang Nhật Linh – Banking / Process Automation Việt Lý – AWS Partner / Cloud \u0026amp; AI Key Highlights 1. Community Vision Opening Remarks: Mr. Nguyễn Gia Hưng kicked off the event at Bitexco Tower, outlining the vision for the AWS community and the objectives of sharing practical, hands-on cloud knowledge. 2. Enterprise \u0026amp; Contextual AI Enterprise Chatbot with MCP: The team presented on unlocking context using the Model Context Protocol (MCP) on AWS. They demonstrated how to build chatbots that handle complex enterprise context effectively, sharing lessons learned from actual implementation. 3. Real-World GenAI Applications Kitchen Recipe Recommendation: An academic showcase featuring a personalized system powered by GenAI. The session detailed the AWS workflow design used to tailor recipes to user preferences. Internal Chatbot with RAG: A deep dive into building an FAQ and knowledge base bot using Retrieval-Augmented Generation (RAG). The key takeaway was the fully serverless architecture, ensuring cost-effectiveness and scalability. 4. Advanced Automation in Banking Multi-Agent Systems: A highlight session on applying GenAI Multi-Agent Systems to automate complex banking processes. The speakers shared real-world case studies demonstrating how multiple agents coordinate to handle specific banking tasks using serverless workflows. 5. Tools \u0026amp; Orchestration GenAI with Kiro IDE \u0026amp; Strands Agent: Explored the application of GenAI in both Production and R\u0026amp;D environments. The session focused on workflow orchestration and featured a live multi-agent demo on AWS. Key Takeaways Architectural Shift From Chatbots to Agents: The industry is moving beyond simple Q\u0026amp;A bots towards Multi-Agent Systems that can execute complex tasks and workflows, especially in regulated sectors like Banking. Serverless is Key: Both RAG and Agent systems were heavily demonstrated on Serverless AWS infrastructure, highlighting it as the standard for modern AI deployment. Practical Implementation Context is King: For enterprise adoption, handling context (via MCP or RAG) is crucial for relevance and accuracy. Orchestration: Tools like Kiro IDE and Strands Agent are emerging to help manage the complexity of AI workflows. Applying to Work Evaluate Multi-Agent Systems: Investigate using a multi-agent approach for complex internal automation tasks rather than a single monolithic model. Adopt Serverless RAG: Review current internal knowledge bases and prototype a Serverless RAG solution to improve information retrieval. Explore MCP: Research the Model Context Protocol to see if it improves context retention in our current chatbot applications. Event Experience The event at Bitexco Tower provided a dense morning of technical insights. The parallel sessions allowed for a deep dive into specific industry verticals.\nNetworking: The \u0026ldquo;Welcome Coffee\u0026rdquo; and \u0026ldquo;Buffer\u0026rdquo; sessions provided excellent opportunities to connect with AWS Solution Architects and industry peers. Practical Demos: Seeing live demos of Multi-Agent systems in banking and Recipe Recommendations bridged the gap between GenAI theory and tangible product delivery. Some event photos Add your event photos here\n"},{"uri":"https://datngo196.github.io/Internship_Report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"MAPVIBE - AI-Powered Map Location Discovery Platform (Discover dining and other locations in Ho Chi Minh City using natural-language prompts and contextual insights)\n1. Executive Summary MapVibe is an AI-driven web platform launched in Ho Chi Minh City to transform location discovery, enabling users to find venues through natural-language prompts (e.g., “find a luxury rooftop restaurant with city view open until midnight” or “quiet coffee shop near the river with outdoor seating”). The platform harnesses Amazon Bedrock’s Large Language Models (LLMs) to interpret user intent, integrating real-time contextual factors like location, time, and preferences, and retrieves data from an internal DynamoDB database. Built on a serverless AWS architecture, MapVibe delivers low latency (\u0026lt;10s), high accuracy (≥85% match satisfaction), and cost efficiency (\u0026lt;$200 for initial 8-week development and demo cycle, completed by October 22, 2025). Authenticated users enjoy personalized recommendations, the ability to contribute reviews, and access to moderation tools, all enhanced by AI technologies.\n2. Problem Statement What’s the Problem? Traditional map platforms like Google Maps rely on keyword-based searches and static filters, struggling to interpret nuanced, context-rich queries (e.g., “quiet coffee shop near the river with outdoor seating”). Users waste time navigating multiple apps to find suitable dining or activity locations. Existing solutions lack conversational interfaces and fail to incorporate contextual signals like time, mood, or group size. The Solution MapVibe employs AWS Bedrock LLMs to parse natural-language prompts in Vietnamese and English, converting them into structured queries. It retrieves and ranks results from an internal DynamoDB database with geo-indexed place data, offering a hybrid interface (conversational search + category filters). User-generated content (reviews, place suggestions) is moderated using AWS Rekognition, ensuring safety and quality through advanced AI-driven analysis.\nBenefits and ROI Speed: Reduces location discovery time from minutes to seconds. Personalization: Context-aware results based on user preferences and behavior, powered by AI. Automation: Eliminates manual filtering with AI-driven intent parsing. Scalability: Global AWS infrastructure ensures low latency and resilience. Cost Efficiency: Optimized to fit within a $200 budget for the initial 8-week cycle, completed by October 22, 2025. Commercial Potential: Opportunities for partnerships with local businesses or integration with internal booking systems. 3. Solution Architecture Overview User Prompt + Context → Bedrock LLM Intent Parsing → Structured Query → DynamoDB Search → Rank \u0026amp; Cache → Web UI Display → User Feedback Loop.\nAWS Services Used Service Function Amazon Route 53 Domain routing AWS Certificate Manager SSL/TLS certificates AWS WAF Web application firewall Amazon CloudFront Global CDN for static assets Amazon API Gateway Secure RESTful API endpoints AWS Lambda Intent parsing, search, and ranking logic Amazon DynamoDB Geo-indexed place data and query caching Amazon S3 Storage for photos, logs, and assets Amazon Cognito User authentication and authorization Amazon Bedrock LLM for intent parsing and summarization Amazon Rekognition AI-driven content moderation for user uploads Amazon EventBridge Scheduled analytics and badge updates Amazon CloudWatch Monitoring and logging Component Design Frontend: Responsive web app (Next.js, bilingual VI/EN, hybrid search UI). Data Ingestion: Prompts and context processed via API Gateway; user uploads (reviews, photos) moderated by Rekognition’s AI. Data Storage: DynamoDB for place data and cached queries (24-hour TTL); S3 for photos and logs. Data Processing: Lambda microservices handle Bedrock LLM calls, query execution, and result ranking. User Management: Cognito for JWT-based authentication (email/social login); guest users access limited features. Output: Displays place cards with AI-generated summaries, ratings, photos, and CTAs (e.g., Get Directions, Call). 4. Technical Implementation Implementation Phases Phase Description Duration 1 Define architecture, Bedrock prompt schema, and DynamoDB schema 2 weeks 2 Estimate costs and optimize caching strategy 1 week 3 Build backend (Lambda, DynamoDB, Bedrock, Rekognition) 3 weeks 4 Develop frontend (Next.js, bilingual, responsive UI) 3 weeks 5 Test and optimize for \u0026lt;10s latency and scalability 2 weeks 6 Launch MVP, deploy via CI/CD, collect feedback 2 weeks Technical Requirements Edge Devices: Modern browsers (Chrome, Safari, Firefox) with PWA-ready responsive UI. Cloud: AWS Route 53, ACM, WAF, CloudFront, API Gateway, Lambda, DynamoDB, S3, Cognito, Bedrock, Rekognition, EventBridge, CloudWatch. Tools \u0026amp; Frameworks: Next.js (App Router), TypeScript, AWS CDK for infrastructure-as-code, GitHub Actions for CI/CD. 5. Timeline \u0026amp; Milestones Period Activities Pre-Development (Month 0 - Sept 2025) Research Ho Chi Minh City venue datasets for DynamoDB Month 1 (Oct 2025) Build backend MVP with Bedrock LLM and DynamoDB Month 2 (Nov 2025) Implement caching, develop frontend integration Month 3 (Nov 2025) Launch public beta, optimize performance, collect feedback Post-Launch (Dec 2025) Add advanced features (e.g., ML-based ranking, offline mode) 6. Budget Estimation Cloud Infrastructure Costs AWS Service Cost/Month (USD) Description Lambda 15 API + LLM logic DynamoDB 10 Cached query store S3 5 Logs, static files API Gateway 10 Request routing Cognito 5 Auth MAU CloudFront 10 Hosting/CDN Bedrock (LLM tokens) 15 Prompt parsing Rekognition 5 Batch image moderation CloudWatch 5 Error-only logging Total ≈ 80/month ≈ 160/8 weeks Cost Optimization Measures Free-Tier Utilization: Leverage AWS free tiers for Lambda, DynamoDB, S3, CloudFront, Rekognition, and Cognito to minimize costs. Aggressive Caching for Bedrock: Achieve a 95% cache hit rate to reduce AI token costs from $120 to \u0026lt;$15/month. Batch Rekognition Processing: Non-real-time image checks save ~$80 over 8 weeks. Simplified Load Testing: 100 users × 10 min scenario instead of 300 × 30 min reduces compute costs. Reduced CloudWatch Logging: Error-only logs save $50+ over 8 weeks. No Provisioned Concurrency: Avoids idle Lambda costs. Environment Variables: Use instead of Secrets Manager to eliminate secret storage charges. On-Demand DynamoDB Mode: All reads/writes free under tier. Disabled Origin Shield: Saves CloudFront overhead. Static Asset Caching: Minimizes outbound data transfer costs. Recommended Budget Scenarios To ensure the MapVibe platform operates efficiently within the $200 AWS budget over the initial 8-week development and demo cycle (completed by October 22, 2025), we recommend the following scenarios based on varying levels of optimization and resource usage:\nMinimal Scenario: Focuses on essential features with maximum reliance on free tiers. This includes disabling non-critical services like WAF if not needed, limiting Bedrock invocations to cached queries only (targeting 98%+ cache hit rate), and conducting no load testing. Estimated cost: \u0026lt;$50 over 8 weeks. Suitable for initial prototyping but may compromise demo reliability due to potential untested scalability issues.\nRecommended Scenario: Balances cost and reliability by incorporating all key optimization measures listed above. This scenario utilizes aggressive caching (95% hit rate for Bedrock), batch processing for Rekognition, simplified load testing (100 users × 10 min), and error-only logging in CloudWatch. It ensures low latency and resilience while staying well under budget. Estimated cost: ~$100-150 over 8 weeks. Ideal for the MVP demo launched on October 22, 2025, providing a robust experience without unnecessary expenses.\nEnhanced Scenario: Includes additional provisions for higher usage post-launch, such as provisioned concurrency for Lambda during peak times and full logging in CloudWatch for detailed debugging. This increases costs slightly but enhances performance monitoring and scalability testing (e.g., 300 users × 30 min loads). Estimated cost: ~$180-200 over 8 weeks. Recommended for ongoing operations after October 22, 2025, if extended demos or higher traffic is anticipated, still within the overall budget cap.\nRecommendation: The Recommended Scenario was successfully implemented for the MVP launch on October 22, 2025, ensuring optimal demo reliability, scalability, and cost control within the $200 budget. For ongoing operations, consider transitioning to the Enhanced Scenario as needed.\nCost Control \u0026amp; Monitoring Create billing alerts and enable AWS Cost Explorer. Tag all resources (Project=MapVibe, Environment=Dev). Review weekly: CloudFront data \u0026gt; 50 GB/week Bedrock cache hit \u0026lt; 90% Lambda invocations \u0026gt; 100K/week Total cost \u0026gt; $15/week 7. Risk Assessment Risk Impact Probability Mitigation DynamoDB data inconsistency High Medium Regular data validation and backups Inaccurate LLM parsing (VN/EN) Medium Low Predefined prompt templates, validation Scalability under high load Medium Medium Serverless auto-scaling, caching Privacy concerns (location data) High Low Explicit user consent, anonymized queries Contingency Plans: Use cached DynamoDB results or local JSON fallback for demos. Implement IP-based rate limits for unauthenticated users.\n8. Expected Outcomes Technical Improvements Conversational Search: Natural-language support for Vietnamese and English with \u0026lt;10s latency, powered by Bedrock LLMs. AI Summaries: Bedrock-generated place overviews, refreshed every 7 days or after 10 new reviews. Scalability: Serverless architecture with global CDN delivery via CloudFront. Moderation: Rekognition’s AI ensures safe user-generated content (reviews, photos). Long-Term Value Personalization: ML-based re-ranking and user behavior analysis. Offline Support: PWA for offline shortlisting of venues. Extensibility: Potential integration with internal booking systems. Contextual Expansion: Recommendations based on weather, events, or social trends. Attachments / References AWS Pricing Calculator GitHub Repository 9. IMPORTANT: Read the SRS to know more about our project Software Requirement Specification Related Documents "},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.2-event2/","title":"AI-Driven Development Workshop","tags":[],"description":"","content":"Summary Report: “AI-Driven Development Workshop” Event Objectives Understand the overview of the AI-Driven Software Development Life Cycle (SDLC) Access and view a real-world demo of Amazon Q Developer Deep dive into the Kiro tool and its application in the development process Update on the latest developer support tools to increase productivity Speakers Toan Huynh – Speaker on AI-Driven SDLC \u0026amp; Amazon Q My Nguyen – Speaker on Kiro Demonstration Key Highlights AI-Driven Development Life Cycle (SDLC) Shift in Paradigm: Shifting from traditional development processes to processes with integrated AI support at every stage. Automation: Automating repetitive tasks in the development lifecycle, from coding to testing and deploying. Efficiency: Optimizing time and resources thanks to intelligent AI assistants. Amazon Q Developer Demonstration Coding Assistant: Demo of code suggestions, code logic explanation, and automatic unit test generation. Troubleshooting: Using Amazon Q to debug and find solutions for technical errors in real-time. Integration: How to integrate Amazon Q into the development environment (IDE) and daily workflows. Kiro Demonstration Tool Capabilities: Introduction to the core features of Kiro (related to Kiro IDE/Agent). Practical Use Cases: Live demo on how to use Kiro to solve specific programming problems. Developer Experience: Improving the developer experience through Kiro\u0026rsquo;s interface and intelligent features. Key Takeaways Tooling Landscape Amazon Q Developer is not just a chat tool but a comprehensive assistant for the SDLC. Kiro brings new approaches to supporting the development environment (IDE/Agent). Productivity Focus Applying AI-Driven SDLC helps minimize manual tasks, allowing developers to focus on complex business logic. It is necessary to proactively get used to these tools to avoid falling behind in new technology trends. Applying to Work Integrate Amazon Q: Install and pilot Amazon Q Developer in current projects to support code reviews and test writing. Explore Kiro: Dedicate time to research Kiro more deeply after the demo to consider its applicability to the team\u0026rsquo;s workflow. Review SDLC Process: Re-evaluate the team\u0026rsquo;s current development process and identify bottlenecks that can be solved with AI. Event Experience The afternoon workshop focused deeply on technical demos, providing an intuitive view of the power of modern programming support tools.\nHands-on Insight Toan Huynh\u0026rsquo;s presentation helped me clearly visualize what an \u0026ldquo;AI-ized\u0026rdquo; SDLC process looks like. My Nguyen\u0026rsquo;s demo on Kiro was very practical, showing the potential of emerging tools alongside giants like AWS. Impact on Workflow It became clear that AI is changing the way we write software: faster, more accurate, and with fewer errors. The event was concise but substantial, going straight into the combat tools that developers care about. Some event photos Add your event photos here\nOverall, the afternoon session was the perfect complement in terms of Tools to the Architecture knowledge gained earlier, completing the picture of modern software development.\n"},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Delve into critical networking services on the AWS platform. Set up and configure a secure Virtual Private Cloud (VPC) environment, including the creation of Public and Private Subnets distributed across multiple Availability Zones (AZs) to ensure a high-availability architecture. Master the Internet connectivity mechanism for resources within the VPC, differentiating the roles of the Internet Gateway (IGW) and the NAT Gateway. Understand and implement AWS network security layers: Network Access Control Lists (NACLs) at the Subnet level and Security Groups (SGs) at the Elastic Network Interface (ENI) level. Research large-scale networking solutions between multiple VPCs, specifically VPC Peering and Transit Gateway. Familiarize with the types of Elastic Load Balancers (ELB), focusing on Layer 7 routing capabilities (ALB) and Layer 4 extreme performance (NLB). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Set up Basic VPC Architecture (Multi-AZ): Create a VPC with a specified CIDR range. Create at least 4 Subnets (Public/Private across 2 different AZs) to ensure a high-availability architecture. Understand the rule of 5 IP addresses reserved by AWS in each Subnet.\n09/08/2025 09/08/2025 3 - Configure Internet Gateway (IGW): Create and attach an IGW to the VPC. Create a Custom Route Table for Public Subnets. Route Internet traffic (0.0.0.0/0) via the IGW. Associate this Route Table with the Public Subnets. 09/09/2025 09/09/2025 4 - Deploy NAT Gateway: Differentiate NAT Gateway and NAT Instance. Allocate an Elastic IP. Deploy the NAT Gateway in a Public Subnet. Configure the Route Table for Private Subnets, routing Internet traffic (0.0.0.0/0) through the NAT Gateway, allowing outbound access only. 09/10/2025 09/10/2025 5 - Configure Security and Deploy EC2: Study and differentiate Security Groups (stateful, ENI level, ALLOW only) and NACLs (stateless, Subnet level, ALLOW/DENY). Create Security Groups for public and private hosts. Deploy EC2 in Public and Private Subnets. Test connectivity between Subnets and to the Internet (using a Bastion Host/Jump Host concept to SSH into Private EC2). 09/11/2025 09/11/2025 6 - Connectivity and ELB: Learn about VPC Peering (1:1 connection, no transitive routing support) and Transit Gateway (Central Hub for connecting a large number of VPCs). Explore Elastic Load Balancing (ELB), focusing on ALB (Layer 7, path-based routing) and NLB (Layer 4, extreme performance, static IP support). 09/12/2025 09/12/2025 Week 2 Achievements: Established a Complete VPC: Successfully created a VPC and divided Subnets into Public/Private tiers across multiple Availability Zones (AZs), ensuring a high-availability architecture. Mastered Internet Outbound/Inbound Mechanism: Configured IGW to allow Public Internet access for Public Subnets. Secured Internet Access for Private Subnets: Deployed a NAT Gateway (placed in a Public Subnet) and configured the corresponding Route Table, allowing instances in the Private Subnet to access the Internet outbound only. Applied Network Layer Security: Differentiated and configured Security Groups (Stateful, applied at ENI) and NACLs (Stateless, applied at Subnet). Successfully deployed EC2 Instances in both Public and Private Subnets and verified internal and external connectivity using jump hosts (Bastion Host) Researched Large-Scale Connectivity: Understood the functional difference between VPC Peering (1:1) and the scalable Transit Gateway (Hub-and-spoke). Familiarized with Elastic Load Balancer types (ALB for Layer 7 routing, NLB for Layer 4 extreme performance) and core features "},{"uri":"https://datngo196.github.io/Internship_Report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.3-event3/","title":"AWS AI/ML &amp; GenAI Workshop","tags":[],"description":"","content":"Summary Report: “AWS AI/ML \u0026amp; GenAI Workshop” Event Objectives Provide an overview of the AI/ML landscape and trends in Vietnam Guide hands-on practice on Amazon SageMaker for building end-to-end ML models Deep dive into Generative AI with Amazon Bedrock (Foundation Models, Agents, Guardrails) Equip attendees with Prompt Engineering skills and RAG (Retrieval-Augmented Generation) application building Speakers AWS Experts Team (Specific speaker names were not listed, but the session was led by technical experts) Key Highlights Welcome \u0026amp; Introduction Landscape Overview: Update on the panorama of Artificial Intelligence and Machine Learning (AI/ML) in the Vietnam market. Networking: Ice-breaker activity to create an open atmosphere for the workshop. AWS AI/ML Services Overview (SageMaker) End-to-end ML Platform: Understand the workflow on Amazon SageMaker from Data preparation, Labeling, to Training and Tuning models. MLOps Integration: Integrating Machine Learning Operations (MLOps) to automate deployment. SageMaker Studio Demo: Experience the interface and features of SageMaker Studio directly through a live walkthrough. Generative AI with Amazon Bedrock Foundation Models Selection: Comparison and guide on selecting suitable foundation models like Claude, Llama, Titan. Prompt Engineering: Optimization techniques: Chain-of-Thought, Few-shot learning. RAG Architecture: \u0026ldquo;Retrieval-Augmented Generation\u0026rdquo; architecture and how to integrate Knowledge Bases to increase AI accuracy. Advanced Features: Using Bedrock Agents for multi-step workflows and Guardrails for content safety. Live Demo: Building a complete GenAI Chatbot using Amazon Bedrock right in the class. Key Takeaways Platform Capabilities SageMaker is a powerful tool for traditional Machine Learning tasks, standardizing the process from data to model. Bedrock provides the fastest shortcut to access Generative AI via API without managing complex infrastructure. Strategic Implementation RAG \u0026amp; Agents are two key technologies that help GenAI applications solve complex business problems rather than just simple chatting. Guardrails are an indispensable component to ensure AI operates within safety frameworks and complies with corporate regulations. Applying to Work Implement MLOps: Apply standard processes on SageMaker to manage model lifecycles in current projects. Build RAG Systems: Experiment with integrating internal documents into Bedrock Knowledge Base to create information retrieval assistants. Optimize Prompts: Apply Chain-of-Thought techniques to improve the response quality of existing chatbots. Model Evaluation: Use the learned criteria to select the most suitable model (Claude vs Llama) in terms of cost and performance for each use case. Event Experience The workshop was a balanced combination of traditional Machine Learning and modern Generative AI, providing a solid foundational knowledge.\nHands-on \u0026amp; Demo The SageMaker Studio walkthrough helped me clearly visualize a professional working environment for Data Scientists. The Building a Chatbot with Bedrock demo was a highlight, proving that creating GenAI applications has become easier and faster than ever. Market Insight The introduction to the AI landscape in Vietnam helped me position my business within the general trend and identify potential opportunities. Some event photos Add your event photos here\nOverall, this event equipped me with a full \u0026ldquo;toolkit\u0026rdquo;: from SageMaker for Predictive models to Bedrock for Generative models, ready for upcoming AI projects.\n"},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Successfully build the Hybrid DNS network architecture to integrate the On-Premise DNS system (simulated by AWS Managed Microsoft Active Directory) with Amazon Route 53 DNS service. Successfully configure Inbound Endpoint, Outbound Endpoint, and Resolver Rules for bidirectional DNS query routing. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Preparation and Network Infrastructure Initialization: Review Route 53 Resolver features. Create an EC2 Key Pair (e.g., hr-dns-key) for secure Remote Desktop access. Initialize the CloudFormation Template (e.g., hr-dns-vpc-stack) to deploy the foundational high-availability and secure network infrastructure (VPC, Subnets, Gateways). 09/15/2025 09/15/2025 3 - Infrastructure Finalization \u0026amp; RDGW Connection: Monitor CloudFormation stack completion. Reconfigure the VPC Security Group (SG): remove unused ports (3391, 443) and retain ICMP and RDP (3389). Connect to the Remote Desktop Gateway (RDGW) Host by downloading the RDP file, uploading the Key Pair to decrypt, and retrieving the Administrator password. 09/16/2025 09/16/2025 4 - Route 53 Outbound Endpoint Setup: Create the Route 53 Outbound Endpoint to allow Route 53 Resolver to forward DNS queries externally (to AD). Select the correct VPC and corresponding Security Group. Configure automatic IP addresses in two different Availability Zones. Wait for the endpoint status to become operational. 09/17/2025 09/17/2025 5 - Resolver Rules and Inbound Endpoint Setup: Create a Resolver Rule (type Forward) to forward DNS queries for the specific domain (e.g., onprem.example.com) to the AWS Managed Microsoft AD DNS IP addresses. Create the Route 53 Inbound Endpoint to allow the On-Premise DNS system (AD) to query the Route 53 Resolver, configuring it within private subnets. 09/18/2025 09/18/2025 6 - Testing and Resource Cleanup: Test results using the nslookup onprem.example.com command on the RDGW host. Verify that the query is resolved via the VPC DNS Resolver IP (VPC CIDR + 2, e.g., 10.0.0.2). Clean up resources: Delete Inbound/Outbound Endpoints. Disassociate the VPC from the Resolver Rule before deleting the Rule. Delete AWS Managed Microsoft Active Directory, and finally, delete the CloudFormation Stacks. 09/19/2025 09/19/2025 Week 3 Achievements: Successfully implemented the Hybrid DNS architecture using Route 53 Resolver. Configured the Outbound Endpoint to allow Route 53 Resolver to forward DNS queries externally (to AWS Managed AD). Created the Inbound Endpoint enabling the On-Premise DNS system (AD) to query the Route 53 Resolver. Resolver Rules (Forward type) were successfully established to route queries for onprem.example.com to the AWS Managed Microsoft AD DNS IP addresses. Testing using nslookup on the RDGW host confirmed successful bidirectional DNS resolution. Verified that queries were resolved via the VPC DNS Resolver IP (e.g., 10.0.0.2, defined as VPC CIDR + 2). Completed comprehensive resource cleanup to prevent unexpected costs. "},{"uri":"https://datngo196.github.io/Internship_Report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://datngo196.github.io/Internship_Report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs I have translated:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.4-event4/","title":"AWS DevOps &amp; Modern Operations","tags":[],"description":"","content":"Summary Report: “AWS DevOps \u0026amp; Modern Operations” Event Objectives Build a DevOps mindset and master key efficiency metrics (DORA metrics) Establish a complete CI/CD process using AWS Developer Tools Modernize infrastructure management with Infrastructure as Code (IaC) using CloudFormation and CDK Deploy containerized applications (Docker) on ECS, EKS, and App Runner Set up a comprehensive Observability system for distributed applications Speakers AWS Experts Team (Specialists in DevOps, Containers, and Observability) Key Highlights DevOps Mindset \u0026amp; CI/CD DORA Metrics: Understanding the importance of Deployment Frequency, Lead Time for Changes, MTTR, and Change Failure Rate. Git Strategies: Comparison of GitFlow and Trunk-based development strategies. Pipeline Automation: Demo of a full CI/CD pipeline from CodeCommit (source), CodeBuild (build/test) to CodeDeploy (deployment), orchestrated by CodePipeline. Deployment Strategies: Safe deployment techniques: Blue/Green, Canary, and Rolling updates. Infrastructure as Code (IaC) CloudFormation: Managing infrastructure via templates, concept of Stacks, and Drift detection. AWS CDK: Using familiar programming languages to define infrastructure, leveraging \u0026ldquo;Constructs\u0026rdquo; and reusable patterns. IaC Choice: Discussion on criteria for choosing between CloudFormation and CDK depending on the project. Container Services Spectrum of Compute: From image management (ECR) to orchestration options: ECS (simplified), EKS (standard Kubernetes), and App Runner (maximum simplification). Microservices Deployment: Comparison and demo of microservices deployment on different platforms. Monitoring \u0026amp; Observability Full-stack Observability: Combining CloudWatch (Metrics, Logs, Alarms) and X-Ray (Distributed Tracing) for a comprehensive view of system health. Best Practices: Setting up Monitoring Dashboards and effective On-call processes. Key Takeaways Automation First CI/CD is not just a toolset but a culture that minimizes human error and accelerates release velocity. IaC is a mandatory standard for modern infrastructure, ensuring consistency across Dev/Test/Prod environments. Operational Excellence Observability is more critical than simple Monitoring, especially in Microservices architectures for error tracing. Choosing the right deployment strategy (like Blue/Green) helps reduce Downtime to zero. Applying to Work Refactor Pipeline: Transition current manual build processes to AWS CodePipeline with automated testing steps. Adopt CDK: Start using AWS CDK to define infrastructure for new projects instead of manual Console operations. Containerization: Dockerize applications and pilot deployment on AWS App Runner for smaller services. Setup Tracing: Integrate AWS X-Ray into applications to monitor latency between microservices. Event Experience The full-day event was intense but the content was very cohesive, moving logically from Mindset to Tools and Operations.\nIntegrated Workflow The \u0026ldquo;Full CI/CD pipeline walkthrough\u0026rdquo; demo was impressive, showing the complete picture of how code moves from a developer\u0026rsquo;s machine to Production. Understanding the clear differences and specific use cases for ECS vs EKS gave me confidence in proposing solutions for the company. Practical Focus Lessons on Deployment strategies (Feature flags, Canary) were very practical for solving the team\u0026rsquo;s \u0026ldquo;deployment fear.\u0026rdquo; The Career roadmap at the end provided clear direction for developing DevOps skills. Some event photos Add your event photos here\nOverall, the workshop systematized the entire body of knowledge regarding modern operations, helping me understand the tight coupling between Code, Infrastructure, and Monitoring.\n"},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Deep dive into AWS Compute services (EC2) including scaling and storage options. Implement data protection strategies using AWS Backup. Configure Hybrid Cloud Storage using AWS Storage Gateway. Master Amazon S3 for static website hosting, content delivery (CloudFront), and data management (Versioning/Replication). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study EC2 core concepts including Instance types, AMIs, Key Pairs, differences between EBS and Instance Store, and practice launching an instance with User Data configuration 09/22/2025 09/22/2025 3 - Learn about EC2 Auto Scaling and Pricing options, then proceed to Lab 13 to deploy AWS Backup infrastructure, create a backup plan, and test the restoration process. 09/23/2025 09/23/2025 4 - Complete Lab 24 by setting up an EC2 for Storage Gateway, activating the Gateway, and creating File Shares. Start Lab 57 by creating an S3 bucket and loading initial data. 09/24/2025 09/24/2025 5 - Continue Lab 57 by enabling Static Website Hosting on S3, configuring public access settings, and setting up an Amazon CloudFront distribution to serve the website content. 09/25/2025 09/25/2025 6 - Finalize Lab 57 by exploring advanced features like Bucket Versioning, Lifecycle rules (moving objects), and Multi-Region Replication, followed by a complete resource cleanup. 09/26/2025 09/26/2025 Week 4 Achievements: Compute (EC2): Understood the differences between EBS and Instance Store. Learned how to use User Data to bootstrap instances. Configured EC2 Auto Scaling groups and understood pricing models. Data Protection: Successfully deployed AWS Backup to automate data backup and restoration procedures. Hybrid Storage: Created and configured AWS Storage Gateway to map file shares to cloud storage. Storage \u0026amp; CDN (S3 \u0026amp; CloudFront): Hosted a static website on Amazon S3. Integrated Amazon CloudFront for content delivery/caching. Implemented data management features: Versioning, Cross-Region Replication, and Lifecycle policies. "},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in five events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS First Cloud Journey Community Day\nDate \u0026amp; Time: 09:00, August 30, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Workshop\nDate \u0026amp; Time: 09:00, October 03, 2025\nLocation: Bitexco Tower (Floor 26 \u0026amp; 36), 02 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS AI/ML \u0026amp; GenAI Workshop\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: Bitexco Tower, 02 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS DevOps \u0026amp; Modern Operations\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: Bitexco Tower, 02 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: AWS Security Specialty Workshop\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: Bitexco Tower, 02 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nEvent 6 Event Name: Workshop: Data Science on AWS\nDate \u0026amp; Time: 09:30, October 16, 2025\nLocation: Hall Academic – FPT University, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.5-event5/","title":"AWS Security Specialty Workshop","tags":[],"description":"","content":"Summary Report: “AWS Security Specialty Workshop” Event Objectives Deeply understand the role of the Security Pillar within the Well-Architected Framework Master the 5 security pillars: IAM, Detection, Infrastructure, Data Protection, Incident Response Update on top security threats in the Vietnam cloud market Practice reviewing permissions (IAM) and building Incident Response (IR) Playbooks Speakers AWS Security Experts Team (Specialized in security architecture and compliance) Key Highlights Foundation \u0026amp; Identity (Pillar 1) Core Principles: Strictly applying Least Privilege, Zero Trust, and Defense in Depth principles. Modern IAM: Shifting from IAM Users (long-term credentials) to IAM Roles and AWS Identity Center (SSO) for centralized management. Access Control: Using Service Control Policies (SCPs) and Permission Boundaries to limit permission scopes in multi-account environments. Mini Demo: Practice Validating IAM Policies and simulating access to detect security flaws. Detection \u0026amp; Infrastructure (Pillar 2 \u0026amp; 3) Continuous Monitoring: Enabling CloudTrail (org-level), GuardDuty, and Security Hub for continuous monitoring. Logging Strategy: Logging at every layer: VPC Flow Logs (network), ALB logs (application), S3 logs (storage). Network Security: Network segmentation with VPC, combining Security Groups and NACLs. Edge protection with WAF, Shield, and Network Firewall. Data Protection \u0026amp; Incident Response (Pillar 4 \u0026amp; 5) Encryption: Encrypting data in-transit and at-rest on S3, EBS, RDS using KMS. Secrets Management: Eliminating hard-coded credentials by using Secrets Manager and Parameter Store with rotation mechanisms. IR Automation: Building Playbooks for common incidents (compromised keys, malware) and automating isolation processes using Lambda/Step Functions. Key Takeaways Zero Trust Mindset Identity is the new perimeter: In the Cloud environment, Identity is the most critical defense barrier, not IP addresses. Never trust by default; always verify and grant least privilege. Automation is Key Manual security cannot keep up with the speed of the Cloud. Detection-as-Code and Auto-remediation must be applied to minimize human risk. Applying to Work Review IAM: Audit all IAM Users, delete old keys, and switch to IAM Roles for applications. Enable GuardDuty: Activate GuardDuty across all regions/accounts to detect anomalous behaviors. Implement Secrets Manager: Replace config files containing DB passwords with API calls to Secrets Manager. Draft IR Playbook: Write an incident response procedure for \u0026ldquo;Compromised IAM Key\u0026rdquo; scenarios and rehearse with the team. Event Experience The workshop dove deep into technical details, comprehensively covering the security aspects that a Cloud Engineer needs to master.\nComprehensive Framework Structuring content by the 5 Pillars helped me systematize previously scattered security knowledge into a standard framework. The section on Top threats in Vietnam was very practical, helping identify specific risks within the local context. Practical Demos The demos on Access Analyzer and Validate IAM Policy were extremely useful, solving the daily headache of debugging permissions. The Incident Response section helped me understand that \u0026ldquo;detection\u0026rdquo; is only half the story; automated \u0026ldquo;response\u0026rdquo; is the goal. Some event photos Add your event photos here\nOverall, the event confirmed that security is not a blocker but an enabler that allows businesses to operate faster and more securely.\n"},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Master advanced Amazon S3 features (Storage Classes, Lifecycle, Access Points) and Glacier. Understand Hybrid Cloud solutions using AWS Storage Gateway and the Snow Family. Learn to migrate on-premises Virtual Machines to AWS (VM Import/Export). Deploy and manage Windows File Server file systems (FSx) with Multi-AZ configurations. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch Module 04 theory videos covering S3 Storage Classes, Access Points, Static Website Hosting, CORS, and the Snow Family, then start Lab 13 by creating S3 buckets and deploying backup infrastructure. 09/29/2025 09/29/2025 3 - Complete Lab 13 by setting up notifications and testing the restore process, then begin Lab 14 by preparing VMWare Workstation and exporting a virtual machine from on-premises environment.\n09/30/2025 09/30/2025 4 - Continue Lab 14 by uploading the VM to AWS, importing it as an AMI, and launching an instance; proceed to Lab 24 to initialize a Storage Gateway service. 10/01/2025 10/01/2025 5 - Finalize Lab 24 by creating File Shares and mounting them on a local machine, then start Lab 25 to create SSD and HDD Multi-AZ file systems (FSx for Windows File Server). 10/02/2025 10/02/2025 6 - Complete the setup of Multi-AZ file systems in Lab 25, review all storage concepts learned during the week, and perform a thorough cleanup of all created resources (S3, Gateways, File Systems). 10/03/2025 10/03/2025 Week 5 Achievements: Advanced Storage: Deepened understanding of S3 performance, security (CORS/ACL), and archival strategies with Glacier. Explored the AWS Snow Family for offline data transfer. Backup \u0026amp; Migration: Successfully configured AWS Backup notifications and tested data restoration. Performed a \u0026ldquo;Lift and Shift\u0026rdquo; migration by importing a VMWare virtual machine into AWS EC2. Hybrid \u0026amp; File Systems: Bridged on-premise and cloud storage using AWS Storage Gateway. Deployed highly available Windows File Systems (FSx) with Multi-AZ SSD/HDD configurations. "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.6-event6/","title":"Workshop: Data Science on AWS","tags":[],"description":"","content":"Summary Report: “Workshop: Data Science on AWS” Event Objectives Explore the complete journey of building a modern Data Science system from theory to practice. Understand the end-to-end Data Science Pipeline on AWS, from storage to processing and deployment. Gain hands-on experience with real-world datasets (IMDb) and practical models (Sentiment Analysis). Analyze the trade-offs between Cloud and On-premise infrastructures regarding cost and performance. Speakers Mr. Văn Hoàng Kha – Cloud Solutions Architect, AWS Community Builder Mr. Bạch Doãn Vương – Cloud DevOps Engineer, AWS Community Builder Key Highlights 1. Cloud in Data Science \u0026amp; Pipeline Overview Importance of Cloud: Discussed why modern data science relies on the cloud for scalability and integration, moving away from rigid on-premise constraints. The AWS Data Science Pipeline: Storage: Using Amazon S3 as the foundational data lake. ETL/Processing: Utilizing AWS Glue for serverless data integration. [cite_start]Modeling: Leveraging Amazon SageMaker as the central hub for building, training, and deploying models[cite: 212]. [cite_start]The session overviewed the broad AWS AI/ML stack, which encompasses AI services, ML services, and infrastructure[cite: 201]. 2. Practical Demos Demo 1: Data Processing with AWS Glue: Scenario: Processing and cleaning raw data from the IMDb dataset. Technique: Demonstrated how to handle feature engineering and data preparation efficiently. [cite_start]The workshop highlighted different approaches, ranging from low-code options like SageMaker Canvas [cite: 209] [cite_start]to code-first methods using Numpy/Pandas[cite: 210]. Demo 2: Sentiment Analysis with SageMaker: Scenario: Training and deploying a machine learning model to analyze text sentiment. [cite_start]Workflow: Showcased the \u0026ldquo;Train, Tune, Deploy\u0026rdquo; lifecycle within SageMaker Studio[cite: 212]. [cite_start]The session also touched upon \u0026ldquo;Bring Your Own Model\u0026rdquo; (BYOM) concepts, demonstrating flexibility with frameworks like TensorFlow and PyTorch[cite: 213]. 3. Strategic Discussions Cloud vs. On-Premise: A deep dive into cost optimization and performance metrics. The discussion highlighted how cloud elasticity allows for experimenting with heavy workloads without the massive upfront capital expenditure of on-premise hardware. Mini-Project Guidance: Introduction to a post-workshop project designed to reinforce the skills learned. Key Takeaways Technical Workflow Unified Pipeline: A robust data science workflow is not just about the code; it requires seamless integration between storage (S3), cleaning (Glue), and modeling (SageMaker). [cite_start]Tool Selection: Understanding when to use managed services (like Amazon Comprehend or Textract [cite: 202, 203]) versus building custom models on SageMaker is crucial for efficiency. Industry Application Real-world Context: The transition from academic theory to industry application lies in automation and scalability. Cost Awareness: Successful data projects must balance model accuracy with computational costs. Applying to Work Adopt AWS Glue: Migrate local ETL scripts to AWS Glue for automated, serverless data cleaning on larger datasets. SageMaker Deployment: Move experimental models from local Jupyter notebooks to SageMaker Studio to standardize the training and deployment process. Project Implementation: Execute the suggested mini-project to solidify understanding of the IMDb processing workflow. Event Experience The “Data Science on AWS” workshop was a bridge between university curriculum and enterprise reality.\nDirect Connection: It connected academic knowledge with the technologies used by top global enterprises. Practical Insight: Watching the IMDb dataset being cleaned and a Sentiment Analysis model being deployed live demystified the complexity of cloud-based AI. Expert Guidance: The interaction with AWS Community Builders provided deep insights into the \u0026ldquo;Cloud vs. On-premise\u0026rdquo; debate, helping me understand the strategic value of cloud migration beyond just technical features. Some event photos Add your event photos here\n"},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Master advanced administration of Amazon FSx for Windows File Server (Performance, Deduplication, Quotas). Deploy a global static website using Amazon S3 and Amazon CloudFront. Implement S3 data resiliency strategies (Versioning, Replication, Lifecycle). Understand the fundamentals of AWS Security, Identity, and Access Management (IAM \u0026amp; Cognito). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Deep dive into Lab 25 advanced features including creating file shares, testing and monitoring performance, enabling data deduplication, shadow copies, and managing user sessions and storage quotas. 10/06/2025 10/06/2025 3 - Finalize Lab 25 by practicing throughput and storage scaling, then delete the environment. Start Lab 57 by creating an S3 bucket, loading data, and enabling static website hosting features. 10/07/2025 10/07/2025 4 - Continue Lab 57 by configuring public access blocks and public objects, then set up and test an Amazon CloudFront distribution to serve the static website content with low latency. 10/08/2025 10/08/2025 5 - Complete Lab 57 with data management tasks: implementing Bucket Versioning, moving objects via Lifecycle rules, configuring Multi-Region Replication, and finally cleaning up all Lab resources. 10/09/2025 10/09/2025 6 - Begin Module 05 by studying the AWS Shared Responsibility Model, understanding the core concepts of AWS Identity and Access Management (IAM), and getting an overview of Amazon Cognito. 10/10/2025 10/10/2025 Week 6 Achievements: Advanced File Storage (FSx): Configured storage efficiency features like Data Deduplication and Shadow Copies. Managed user access controls through sessions and storage quotas. Performed scaling operations for throughput and storage capacity. Content Delivery \u0026amp; S3: Successfully hosted a static website on S3 and accelerated delivery using CloudFront CDN. Implemented data protection and lifecycle strategies (Versioning, Replication). Security Fundamentals: Grasped the Shared Responsibility Model between AWS and the customer. Understood the role of IAM for access control and Cognito for user identity management. "},{"uri":"https://datngo196.github.io/Internship_Report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd. from 08/09/2025 to 19/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment. I participated in FCJ Cloud Intern, through which I improved my skills in Cloud Computing architecture, Infrastructure as Code (AWS CDK), DevOps practices (CI/CD), Generative AI applications, and professional communication.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ☐ ✅ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Understand enterprise security structures using AWS Organizations and Identity Center. Implement security posture management with AWS Security Hub and encryption with KMS. Build automated incident response workflows using AWS Lambda and Slack integration. Master resource organization and management using Tagging strategies and Resource Groups. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch theory videos on AWS Organizations, Identity Center, KMS, and Security Hub. Complete Lab 18 by enabling Security Hub to assess security scores and then cleaning up resources. 10/13/2025 10/13/2025 3 - Begin Lab 22 to build an automated response system. Set up the network infrastructure (VPC, Security Groups), launch an EC2 instance, and configure an Incoming Webhook for Slack integration. 10/14/2025 10/14/2025 4 - Finalize Lab 22 by creating IAM Roles for Lambda, deploying functions to automatically stop/start instances, and verifying the automation results through Slack notifications before cleanup. 10/15/2025 10/15/2025 5 - Perform Lab 27 to focus on resource management. Practice launching EC2 instances with tags, managing tags via both Console and CLI, and filtering resources efficiently. 10/16/2025 10/16/2025 6 - Complete Lab 27 by creating Resource Groups based on tags and cleaning up. Start Lab 28 by learning how to create and configure a secure IAM User. 10/17/2025 10/17/2025 Week 7 Achievements: Security Governance: Gained insight into managing multi-account environments with AWS Organizations and Identity Center. Used AWS Security Hub to monitor compliance and security standards. Automation \u0026amp; Integration: Successfully built a serverless automation workflow using AWS Lambda to manage EC2 states. Integrated AWS services with third-party tools (Slack) for real-time monitoring. Resource Management: Applied advanced tagging strategies to organize cloud resources. Utilized Resource Groups and CLI commands for efficient bulk resource management. "},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.7-event7/","title":"Vietnam Cloud Day","tags":[],"description":"","content":"Summary Report: “Vietnam Cloud Day” Event Objectives Provide strategic insights for executive leadership on navigating the Generative AI revolution. Share best practices for building a unified, scalable data foundation on AWS. Introduce the AI-Driven Development Lifecycle (AI-DLC) and its impact on software implementation. Explore security fundamentals for Generative AI and the future of AI Agents. Speakers Eric Yeo – Country GM, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner – CEO, Techcombank Ms. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network Jaime Valles – VP \u0026amp; GM APJ, AWS Jeff Johnson, Vu Van (ELSA), Nguyen Hoa Binh (Nexttech), Dieter Botha (TymeX) – Panelists Kien Nguyen, Jun Kai Loke, Tamelly Lim, Binh Tran, Taiki Dang, Michael Armentano – AWS Specialists Key Highlights 1. Strategic Leadership \u0026amp; Vision Keynotes: Industry leaders from AWS, Techcombank, and U2U Network shared their vision for cloud and AI adoption in the region. Executive Panel: A discussion on \u0026ldquo;Navigating the GenAI Revolution,\u0026rdquo; focusing on fostering an innovation culture, aligning AI with business objectives, and managing organizational change during AI integration. 2. Data Foundation \u0026amp; Roadmap Unified Data Foundation: The session outlined how to build a robust infrastructure handling data ingestion, storage, processing, and governance—a critical prerequisite for advanced analytics and AI workloads. GenAI Roadmap: AWS presented its comprehensive vision and emerging trends to empower organizations to leverage GenAI for efficiency. 3. The Future of Software Development AI-Driven Development Lifecycle (AI-DLC): A transformative approach where AI is not just an assistant but a central collaborator. It integrates AI-powered execution with human oversight to drastically improve speed and innovation, moving beyond traditional methods. 4. Security \u0026amp; Advanced Automation Securing GenAI: Addressed security at three layers: infrastructure, models, and applications. Emphasized built-in measures like encryption, zero-trust architecture, and fine-grained access controls. AI Agents: The closing session highlighted a paradigm shift from basic automation to Intelligent Agents—partners that learn, adapt, and execute complex tasks autonomously. Key Takeaways Cultural Shift AI-DLC: Software development is evolving from \u0026ldquo;human-driven with AI assistance\u0026rdquo; to \u0026ldquo;AI-centric collaboration,\u0026rdquo; requiring a shift in how teams approach coding and testing. Agents vs. Automation: There is a distinct difference between static automation scripts and dynamic AI Agents that can make decisions and adapt to changing inputs. Technical Pillars Data First: You cannot have successful GenAI without a unified and governed data foundation. Security by Design: Security for GenAI must be continuous and layered, ensuring data confidentiality throughout the lifecycle. Applying to Work Assess Data Readiness: Review current AWS data infrastructure to ensure it meets the scalability and governance requirements for GenAI (per the Unified Data Foundation session). Explore AI Agents: Identify manual, complex operational tasks that could be offloaded to autonomous AI Agents rather than simple scripts. Adopt AI-DLC: Experiment with embedding AI tools more deeply into the dev lifecycle to act as collaborators rather than just code completers. Event Experience This summit provided a holistic view of the GenAI landscape, balancing high-level executive strategy with deep technical dives.\nStrategic Insight: The panel with leaders from ELSA, Nexttech, and TymeX offered valuable real-world perspectives on managing the cultural changes AI brings. Technical Depth: The afternoon tracks were particularly useful, specifically the deep dive into AI-DLC and Securing GenAI, which are immediate concerns for our technical roadmap. Some event photos Add your event photos here\n"},{"uri":"https://datngo196.github.io/Internship_Report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment: The environment at FCJ is highly professional and strongly encourages a \u0026ldquo;Builder\u0026rdquo; mindset. The workspace is always filled with positive energy, not only comfortable but also inspiring creativity. I particularly appreciate how everyone debates candidly and constructively, not afraid of friction to find the optimal solution. Additionally, interns like myself can sit at the same table and discuss directly with mentors without hierarchical barriers. The open space is a key factor that helps me maintain excitement every day when arriving at the office.\n2. Support from Mentor / Team Admin: My mentor acted not just as a technical guide but as a problem-solving mindset coach. Instead of simply giving \u0026ldquo;right or wrong\u0026rdquo; answers, he often asked, \u0026ldquo;Why did you choose this approach?\u0026rdquo; or \u0026ldquo;If the user base scales 10x, will this still work?\u0026rdquo; This coaching method helped me hone my critical thinking and be more meticulous with every line of code. Support from the admin team was very swift, especially in granting access to Cloud resources and paid tools necessary for work.\n3. Relevance of Work to Academic Major: Knowledge of Operating Systems, Computer Networks, or Data Structures learned at school provided a good foundation, but applying them to actual work here elevated them to a new level. I was exposed to real-world problems regarding Cloud Computing, Microservices, and DevOps – areas where school often stops at theory. However, this is exactly what I liked most because it forced me to apply foundational knowledge to real production problems, helping bridge the gap between a student and a professional engineer.\n4. Learning \u0026amp; Skill Development Opportunities: This internship felt like an intensive career boot camp. Regarding technical skills (Hard skills), I became more proficient in using Git flow, writing Infrastructure as Code (IaC), and debugging distributed systems. As for soft skills, I learned how to work within Agile/Scrum processes, how to write technical documentation that is easy for others to understand, and most importantly, communication skills – knowing how to ask the right questions to the right people at the right time to solve problems fastest.\n5. Company Culture \u0026amp; Team Spirit: The company culture values transparency and the spirit of \u0026ldquo;Knowledge Sharing.\u0026rdquo; When an incident occurs, instead of finding someone to blame, the whole team sits down to conduct a Root Cause Analysis to ensure the error doesn\u0026rsquo;t repeat. Teamwork is also exceptional; I never felt alone when facing difficulties because colleagues were always ready to support, share documentation, or pair-program to help me untangle issues. Tech Talk sessions are great opportunities to bond and learn from each other.\n6. Internship Policies / Benefits: Being supported with an AWS account for unrestricted practice (sandbox environment) is a fantastic benefit for technical roles. Additionally, policies supporting international certification exams are a huge motivation for me to strive harder.\nAdditional Questions Most satisfying aspect: I participated in real projects, deployed code to real environments, and saw the actual impact of the features I built. Area for improvement: The initial technical documentation onboarding process is a bit fragmented, taking quite some time to piece together. Would you recommend it: Definitely YES. This is an ideal environment for those passionate about Cloud and technology, who want to experience real-world working pressure. Suggestions \u0026amp; Expectations Suggestion: Should organize more cross-team \u0026ldquo;Code Review\u0026rdquo; sessions between different intern groups to learn from each other\u0026rsquo;s coding styles and thinking. Future intent: I strongly hope to have the opportunity to become a full-time employee (Fresher/Junior) to continue contributing to unfinished projects and further develop the skills I have just started to grasp. Other comments: Thank you to the company for creating a truly high-quality playground for students. "},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Master advanced IAM concepts: Cross-region access, Role Switching, and Attribute-Based Access Control (ABAC) using Tags. Implement restrictive security policies and test IAM User boundaries. Deploy comprehensive data security and auditing using KMS, CloudTrail, and Amazon Athena. Simulate real-world identity management scenarios with IAM Groups and Admin Roles. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Complete Lab 28 by creating IAM Policies and Roles, practicing Switch Roles to access resources in different regions (Tokyo/N. Virginia) based on Tag compliance. Perform Lab 30 to create restriction policies and test IAM user limits. 10/20/2025 10/20/2025 3 - Begin Lab 33 focused on audit and encryption. Create necessary IAM Policies, Roles, Groups, and Users, then initialize a Key Management Service (KMS) key and prepare an S3 bucket for secure data upload. 10/21/2025 10/21/2025 4 - Finalize Lab 33 by configuring CloudTrail to log API events, setting up Amazon Athena to query those logs, and testing the sharing of KMS-encrypted data on S3 before cleaning up. 10/22/2025 10/22/2025 5 - Start Lab 44 to reinforce IAM structural knowledge. Create IAM Groups and Users, and perform detailed permission checks to understand how policies affect user access rights. 10/23/2025 10/23/2025 6 - Complete Lab 44 by creating an Admin IAM Role and configuring the Switch Role mechanism for privilege elevation. Review all security concepts learned and perform a full resource cleanup. 10/24/2025 10/24/2025 Week 8 Achievements: Advanced Identity Management: Successfully implemented Switch Role mechanisms for secure cross-role access. Enforced access controls based on resource tags (ABAC) across different AWS regions. Managed user permissions effectively using Groups and Restriction Policies. Security \u0026amp; Compliance: Secured data at rest using AWS KMS encryption. Established an audit trail using AWS CloudTrail and analyzed logs using SQL queries in Amazon Athena. "},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.8-event8/","title":"Workshop: Data Science on AWS","tags":[],"description":"","content":"Summary Report: “Kick-off AWS First Cloud Journey Workforce OJT FALL 2025” Summary Report: “Workshop: Data Science on AWS” Event Objectives Officially launch the AWS First Cloud Journey (FCJ) Workforce OJT program for Fall 2025. Connect students with industry leaders from AWS, VNG, and G-Asia Pacific. Provide career orientation in Cloud Computing, DevOps, and GenAI. Share inspiring alumni stories and promote diversity with \u0026ldquo;She in Tech\u0026rdquo;. Speakers Mr. Nguyễn Trần Phước Bảo – Head of Enterprise Relations (School Representative) Mr. Nguyễn Gia Hưng – Head of Solutions Architect, AWS Vietnam Mr. Đỗ Huy Thắng – DevOps Lead, VNG Mr. Danh Hoàng Hiếu Nghị – GenAI Engineer, Renova Ms. Bùi Hồ Linh Nhi – AI Engineer, SoftwareOne Mr. Phạm Nguyễn Hải Anh – Cloud Engineer, G-Asia Pacific Mr. Nguyễn Đồng Thanh Hiệp – Principal Cloud Engineer, G-Asia Pacific Key Highlights 1. Opening \u0026amp; Vision Academic Partnership: Mr. Nguyễn Trần Phước Bảo opened the ceremony, emphasizing the strategic collaboration between the university and enterprises to bridge the gap between academic training and industry needs. Future Orientation: Mr. Nguyễn Gia Hưng (AWS) presented the vision of the \u0026ldquo;First Cloud Journey,\u0026rdquo; outlining how this OJT program serves as a launchpad for future cloud architects in Vietnam. 2. Career Pathways: DevOps \u0026amp; GenAI DevOps Reality: Mr. Đỗ Huy Thắng from VNG shared a realistic view of the DevOps career path, highlighting the essential skills required to survive and thrive in a major tech corporation. FCJ to GenAI: Alumni speakers Danh Hoàng Hiếu Nghị and Bùi Hồ Linh Nhi showcased their rapid evolution from FCJ interns to AI/GenAI Engineers, proving the program\u0026rsquo;s effectiveness. 3. Diversity \u0026amp; Daily Life She in Tech: Ms. Linh Nhi\u0026rsquo;s session highlighted the growing role of women in technology, encouraging female students to pursue technical roles in Cloud and AI. A Day in the Life: Speakers from G-Asia Pacific (Mr. Hải Anh \u0026amp; Mr. Hiệp) provided a transparent look at the daily responsibilities of a Cloud Engineer, from junior tasks to principal-level decision-making. Key Takeaways Career Roadmap Multiple Paths: The cloud industry offers diverse trajectories—from pure infrastructure (Cloud Engineer) to automation (DevOps) and cutting-edge innovation (GenAI). Foundation is Key: Success in specialized roles like GenAI starts with a strong foundation in Cloud Computing concepts gained during this OJT. Professional Mindset Adaptability: The transition from university to enterprise requires a shift in mindset—proactivity and continuous learning are non-negotiable. Community: Networking with mentors and alumni provides a crucial support system for career growth. Applying to Work Set OJT Goals: Define specific technical milestones (e.g., getting certified, mastering a specific AWS service) for the upcoming internship period based on speaker advice. Connect with Mentors: actively engage with the speakers and mentors introduced during the networking session. Explore GenAI: Dedicate time during the OJT to explore Generative AI services on AWS, as recommended by the alumni speakers. Event Experience Held at the Bitexco Financial Tower, the kick-off event set a professional and inspiring tone for the semester.\nAtmosphere: The energy was high, with a strong sense of community between the new cohort, alumni, and industry experts. Inspiration: Hearing former students share their success stories (\u0026ldquo;From FCJ to GenAI Engineer\u0026rdquo;) made the career goals feel tangible and achievable. Some event photos Add your event photos here\n"},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.9-event9/","title":"CloudThinker: Agentic AI &amp; Orchestration on AWS","tags":[],"description":"","content":"Summary Report: “CloudThinker: Agentic AI \u0026amp; Orchestration on AWS” Event Objectives Deep dive into AWS Bedrock Agent Core and its capabilities. Explore real-world use cases for building Agentic Workflows. Understand advanced concepts like Agentic Orchestration and Context Optimization at a technical level (L300). Gain hands-on experience through the CloudThinker Hack workshop. Speakers Mr. Nguyen Gia Hung – Head of Solutions Architect, AWS Mr. Kien Nguyen – Solutions Architect, AWS Mr. Viet Pham – Founder \u0026amp; CEO Mr. Thang Ton – Co-founder \u0026amp; COO, CloudThinker Mr. Henry Bui – Head of Engineering, CloudThinker Mr. Kha Van – Workshop Facilitator Key Highlights 1. AWS Foundation Opening: Mr. Nguyen Gia Hung set the stage for the importance of Agentic AI in the current cloud landscape. Bedrock Agent Core: Mr. Kien Nguyen provided a technical overview of AWS Bedrock Agent Core, explaining how it simplifies the creation of agents that can plan and execute tasks by invoking APIs. 2. Practical Application \u0026amp; Use Cases Building Agentic Workflows: Mr. Viet Pham demonstrated a concrete use case, showing the end-to-end process of designing and deploying an agentic workflow on AWS. CloudThinker Introduction: Mr. Thang Ton introduced the CloudThinker ecosystem and their vision for AI-driven cloud solutions. 3. Deep Dive (Level 300) Agentic Orchestration \u0026amp; Context Optimization: This was the technical core of the morning. Mr. Henry Bui discussed advanced strategies for orchestrating multiple agents and optimizing context within Amazon Bedrock to ensure high accuracy and relevance in complex interactions. 4. Hands-on Experience CloudThinker Hack: Led by Mr. Kha Van, this 60-minute session allowed attendees to get their hands dirty, applying the morning\u0026rsquo;s concepts to build a prototype agent using the CloudThinker framework and AWS services. Key Takeaways Evolution of AI From Chat to Action: The industry is shifting from passive chatbots to active Agents that can perform complex orchestration and execute API calls. Context is Critical: As workflows get more complex, standard context windows aren\u0026rsquo;t enough. \u0026ldquo;Context Optimization\u0026rdquo; strategies are essential to keep costs down and accuracy up. Architecture Orchestration Patterns: Managing multiple agents require a robust orchestration layer to decide which agent handles which part of a user request. Applying to Work Prototype an Agent: Use AWS Bedrock Agent to build a simple internal tool that connects to a company API (e.g., checking leave balance or server status). Study Context Patterns: Research the context optimization techniques shared by Henry Bui to apply to our current RAG implementations. Participate in Hackathons: Encourage the team to join similar hands-on hacks to stay sharp on the latest AWS features. Event Experience This event was highly technical and focused.\nTechnical Depth: The L300 session on Orchestration was particularly valuable for understanding how to scale AI applications beyond simple demos. Interactive: The \u0026ldquo;CloudThinker Hack\u0026rdquo; provided immediate reinforcement of the theoretical knowledge, making it one of the most effective learning sessions. Some event photos Add your event photos here\n"},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Finalize IAM security best practices by comparing Access Keys vs. IAM Roles. Understand AWS Database services (RDS, Aurora, Redshift, ElastiCache) and general database concepts. Deploy a 2-tier architecture web application using Amazon EC2 and Amazon RDS. Perform database operations including backup, restoration, and connectivity via RDP. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Complete Lab 44 by implementing conditional switch roles (IP/Time limits). Proceed to Lab 48 to practice generating IAM Access Keys versus using IAM Roles for EC2, understanding why Roles are more secure. 10/27/2025 10/27/2025 3 - Study Module 06 theory videos covering fundamental database concepts (OLTP/OLAP), Amazon RDS \u0026amp; Aurora architectures, and overview of specialized DBs like Redshift and ElastiCache.\n10/28/2025 10/28/2025 4 - Start Lab 05 (Deploying Web App with RDS) by setting up the network foundation: creating a VPC, configuring EC2 and RDS Security Groups, defining DB Subnet Groups, and launching the EC2 instance. 10/29/2025 10/29/2025 5 - Continue Lab 05 by provisioning an Amazon RDS database instance, configuring the application on EC2 to connect to the database, and verifying the successful deployment of the web application. 10/30/2025 10/30/2025 6 - Finalize Lab 05 by performing database backup and restore operations, then clean up all resources. Begin Lab 43 by learning how to connect to Windows instances using an RDP Client. 10/31/2025 10/31/2025 Week 9 Achievements: IAM Security Mastery: Implemented advanced conditional access policies (Date/IP based). Demonstrated the security benefits of using IAM Roles over long-term Access Keys. Database Implementation: Grasped the differences between various AWS database offerings (Relational vs. Key-value vs. Warehousing). Successfully deployed a managed Relational Database (RDS) inside a VPC. Application Deployment: Connected a web application hosted on EC2 to a backend RDS instance. Performed critical database maintenance tasks like snapshots and restoration. "},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Perform heterogeneous database migrations (SQL Server/Oracle to Aurora MySQL) using AWS SCT and DMS. Troubleshoot complex migration scenarios involving schema conversion, memory pressure, and table errors. Build a Serverless Data Analytics pipeline using Amazon Kinesis, Glue, and Athena. Visualize business intelligence data using Amazon QuickSight. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Begin Lab 43 by connecting via EC2 Fleet Manager, configuring Source databases (SQL Server/Oracle), handling constraints, and preparing the Target Aurora MySQL environment. 11/03/2025 11/03/2025 3 - Continue Lab 43 by using the Schema Conversion Tool (SCT) to convert schemas, creating Migration Tasks and Endpoints, and launching a Serverless Migration process. 11/04/2025 11/04/2025 4 - Finalize Lab 43 by monitoring logs, troubleshooting specific test scenarios (Memory Pressure, Table Errors), validating the migrated data, and cleaning up migration resources. 11/05/2025 11/05/2025 5 - Start Module 07 (Lab 35) by setting up the ingestion layer: creating an S3 Bucket, configuring a Kinesis Data Firehose Delivery Stream, and generating sample data for analysis. 11/06/2025 11/06/2025 6 - Complete Lab 35 by configuring an AWS Glue Crawler to catalog data, performing SQL analysis with Amazon Athena, creating visualizations in QuickSight, and cleaning up resources. 11/07/2025 11/07/2025 Week 10 Achievements: Database Migration: Successfully migrated data from heterogeneous sources (SQL Server, Oracle) to AWS Aurora MySQL. Mastered the AWS Schema Conversion Tool (SCT) and Database Migration Service (DMS). Gained experience in troubleshooting migration failures (memory issues, mapping errors). Data Analytics Pipeline: Built a complete serverless data pipeline: Ingestion (Kinesis) -\u0026gt; Storage (S3) -\u0026gt; Catalog (Glue). Analyzed large datasets using SQL queries in Amazon Athena without managing servers. Business Intelligence: Connected Amazon QuickSight to the data source to create interactive visual reports. "},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Master NoSQL database design and operations using Amazon DynamoDB (Backup, Global Tables). Implement cost allocation strategies using Tagging and Cost Management tools. Utilize AWS developer tools (CloudShell, SDK) for programmatic resource management. Perform visual data preparation, profiling, and cleaning using AWS Glue DataBrew. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Deep dive into Amazon DynamoDB (Lab 39): Explore the console, perform backup/restore operations, and study advanced design patterns for building global serverless applications. 11/10/2025 11/10/2025 3 - Complete Lab 40 focusing on Cost Allocation: Build a database, populate data, apply tagging strategies to track usage, and query cost allocation tags to understand spending patterns. 11/11/2025 11/11/2025 4 - Perform Lab 60 to practice managing AWS resources using command-line interfaces: Amazon CloudShell and the AWS SDK, understanding the difference between Console and programmatic access. 11/12/2025 11/12/2025 5 - Begin Lab 70 for data preparation: Launch a Cloud9 environment, download and upload datasets to S3, set up AWS Glue DataBrew, and run data profiling to understand data quality. 11/13/2025 11/13/2025 6 - Finalize Lab 70 by cleaning and transforming data with DataBrew recipes. Proceed to Lab 72 to ingest, store, and catalog data into the AWS Glue Data Catalog, then clean up all resources. 11/14/2025 11/14/2025 Week 11 Achievements: Serverless Database (NoSQL): Gained proficiency in DynamoDB core concepts and event-driven architectures. Implemented backup strategies for NoSQL data. Cost \u0026amp; Management: Applied effective tagging strategies for granular cost tracking and reporting. Demonstrated ability to interact with AWS via CLI/SDK in CloudShell. Data Engineering: Utilized AWS Glue DataBrew to profile and clean raw data without writing code. Built a foundational data catalog for analytics workflows. "},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Master advanced ETL (Extract, Transform, Load) processes using AWS Glue, DataBrew, and Amazon EMR. Implement real-time analytics with Kinesis and data warehousing with Amazon Redshift. Create professional, interactive Business Intelligence (BI) dashboards using Amazon QuickSight. Final Review: Consolidate knowledge from Modules 1-7 and finalize the end-of-term Worklog. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Perform advanced data transformation tasks in Lab 72 using various AWS Glue methods (Interactive Sessions, GUI, DataBrew) and process big data using Amazon EMR. 11/17/2025 11/17/2025 3 - Continue Lab 72 by running SQL queries with Amazon Athena, performing real-time analytics using Kinesis Data Analytics, and creating initial visualizations in QuickSight. 11/18/2025 11/18/2025 4 - Finalize Lab 72 by automating data serving via AWS Lambda and setting up a robust Data Warehouse using Amazon Redshift for complex query performance. 11/19/2025 11/19/2025 5 - Complete Lab 73 by building, refining, and publishing interactive Business Intelligence dashboards in Amazon QuickSight to visualize key insights.\n11/20/2025 11/20/2025 6 - General Review: Review key concepts from Modules 1 to 7, ensure all cloud resources are deleted to prevent costs, and finalize the complete Term Worklog for submission. 11/21/2025 11/21/2025 Week 12 Achievements: Advanced Data Engineering: Executed complex data transformations using Serverless (Glue) and Cluster-based (EMR) services. Built a Data Warehouse infrastructure using Amazon Redshift. Analytics \u0026amp; BI: Implemented real-time data stream processing with Kinesis. Designed and deployed interactive dashboards on Amazon QuickSight for decision-making support. Course Completion: Successfully reviewed the entire cloud journey (Compute, Storage, Database, Security, Analytics). Completed the final Worklog and cleaned up the AWS environment. "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.1-workshop-overview/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Introduction\u0026rdquo; date: 2025-09-09 weight : 1 chapter : false pre : \u0026quot; 5.1. \u0026quot;\nVPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.2-prerequiste/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Prerequiste\u0026rdquo; date: 2025-09-09 weight : 2 chapter : false pre : \u0026quot; 5.2. \u0026quot;\nIAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Create a gateway endpoint\u0026rdquo; date: 2025-09-09 weight : 1 chapter : false pre : \u0026quot; 5.3.1 \u0026quot;\nOpen the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Test the Gateway Endpoint\u0026rdquo; date: 2025-09-09 weight : 2 chapter : false pre : \u0026quot; 5.3.2 \u0026quot;\nCreate S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.3-s3-vpc/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Access S3 from VPC\u0026rdquo; date: 2025-09-09 weight : 3 chapter : false pre : \u0026quot; 5.3. \u0026quot;\nUsing Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Prepare the environment\u0026rdquo; date: 2025-09-09 weight : 1 chapter : false pre : \u0026quot; 5.4.1 \u0026quot;\nTo prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Create an S3 Interface endpoint\u0026rdquo; date: 2025-09-09 weight : 2 chapter : false pre : \u0026quot; 5.4.2 \u0026quot;\nIn this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Test the Interface Endpoint\u0026rdquo; date: 2025-09-09 weight : 3 chapter : false pre : \u0026quot; 5.4.3 \u0026quot;\nGet the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;On-premises DNS Simulation\u0026rdquo; date: 2025-09-09 weight : 4 chapter : false pre : \u0026quot; 5.4.4 \u0026quot;\nAWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.4-s3-onprem/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Access S3 from on-premises\u0026rdquo; date: 2025-09-09 weight : 4 chapter : false pre : \u0026quot; 5.4. \u0026quot;\nOverview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.5-policy/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;VPC Endpoint Policies\u0026rdquo; date: 2025-09-09 weight : 5 chapter : false pre : \u0026quot; 5.5. \u0026quot;\nWhen you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.6-cleanup/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Clean up\u0026rdquo; date: 2025-09-09 weight : 6 chapter : false pre : \u0026quot; 5.6. \u0026quot;\nCongratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://datngo196.github.io/Internship_Report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://datngo196.github.io/Internship_Report/tags/","title":"Tags","tags":[],"description":"","content":""}]
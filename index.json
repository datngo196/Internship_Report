[{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, I will introduce my Worklog. It details how I approached and completed the tasks, the specific duration of the internship program, and a summary of what I accomplished and learned throughout this period :\nWeek 1: Getting familiar with Cloud Computing, IAM, and Cost Management (AWS Budgets)\nWeek 2: Building Network Infrastructure: Multi-AZ VPC, Security, and Load Balancing\nWeek 3: Implementing Hybrid DNS Architecture with Route 53 Resolver\nWeek 4: Getting started with AWS Console/CLI, EC2, and Basic Storage (S3, Storage Gateway)\nWeek 5: Advanced Storage (S3 Glacier/Snow Family), VM Import, and Windows File Server (FSx)\nWeek 6: Advanced FSx Administration, Content Delivery (CloudFront), and IAM Fundamentals\nWeek 7: Enterprise Security Governance (Organizations, Security Hub) and Automation with Lambda\nWeek 8: Advanced Identity Management (IAM), Encryption (KMS), and System Auditing (CloudTrail/Athena)\nWeek 9: IAM Security Optimization and Web Application Deployment with Relational Database (RDS)\nWeek 10: Database Migration (DMS/SCT) and Building Serverless Data Analytics Pipelines\nWeek 11: NoSQL Databases (DynamoDB), Cost Management, and Data Preparation with Glue DataBrew\nWeek 12: Big Data Analytics (EMR, Redshift), Visualization (QuickSight), and Course Wrap-up\n"},{"uri":"https://datngo196.github.io/Internship_Report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Chứng nhận ISO năm 2025 và CSA STAR hiện đã khả dụng cùng với hai dịch vụ bổ sung *Bởi Chinmaee Parulekar, đăng ngày 17 tháng 9 năm 2025, thuộc chuyên mục Announcements, Foundational (100), *Security, Identity \u0026amp; Compliance\nAmazon Web Services (AWS) đã hoàn tất thành công cuộc kiểm toán mở rộng mà không có bất kỳ phát hiện sai sót nào cho các tiêu chuẩn ISO 9001:2015, 27001:2022, 27017:2015, 27018:2019, 27701:2019, 20000-1:2018, 22301:2019 và Tiêu chuẩn Cloud Security Alliance (CSA) STAR Cloud Controls Matrix (CCM) v4.0. Cuộc kiểm toán được thực hiện bởi EY CertifyPoint, và các chứng chỉ đã được cấp lại vào ngày 13 tháng 8 năm 2025. Mục tiêu của cuộc kiểm toán là giúp AWS mở rộng phạm vi các chứng nhận ISO và CSA STAR để bao gồm thêm hai dịch vụ AWS Resource Explorer và AWS Incident Response. Các tiêu chuẩn ISO này bao phủ các lĩnh vực như quản lý chất lượng, an ninh thông tin, bảo mật đám mây, bảo vệ quyền riêng tư, quản lý dịch vụ và duy trì hoạt động kinh doanh liên tục. Việc đạt được các chứng nhận này thể hiện cam kết của AWS trong việc duy trì các biện pháp kiểm soát an ninh mạnh mẽ và bảo vệ dữ liệu khách hàng trên toàn bộ các dịch vụ của mình.\nTrong cuộc kiểm toán mở rộng này, chúng tôi đã bổ sung thêm hai dịch vụ AWS mới vào phạm vi chứng nhận kể từ lần cấp chứng chỉ gần nhất vào ngày 26 tháng 5 năm 2025. Hai dịch vụ được bổ sung bao gồm:\nAWS Resource Explorer\nAWS Security Incident Response\nĐể xem danh sách đầy đủ các dịch vụ AWS đã được chứng nhận theo tiêu chuẩn ISO và CSA STAR, vui lòng truy cập trang AWS ISO and CSA STAR Certified. Khách hàng cũng có thể truy cập các chứng chỉ này trực tiếp trong AWS Management Console thông qua AWS Artifact.\nChinmaee Parulekar là Quản lý Chương trình Tuân thủ (Compliance Program Manager) tại AWS, với 6 năm kinh nghiệm trong lĩnh vực an ninh thông tin. Cô sở hữu bằng Thạc sĩ Khoa học (Master of Science) chuyên ngành Hệ thống Thông tin Quản lý (Management Information Systems), cùng các chứng chỉ nghề nghiệp như CISA và HITRUST CCSF Practitioner. "},{"uri":"https://datngo196.github.io/Internship_Report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Ngo Huu Dat\nPhone Number: 0911449689\nEmail: datngo2005@gmail.com\nUniversity: FPT University\nMajor: Artificial Intelligence\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Master the core concepts of Cloud Computing as defined by AWS Understand the AWS Global Infrastructure, including Regions, Availability Zones (AZs), and Edge Locations Familiarize with fundamental management tools (Console, IAM) and initial cost optimization methods on AWS Complete initial account setup, implementing security (MFA, IAM User), and budget management (AWS Budget) Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Research Cloud definition, payment models, core benefits (cost optimization, elasticity, global scaling). Research AWS Global Infrastructure (Data Center, AZ, Region, Edge Locations) 09/01/2025 09/01/2025 3 - Hands-on practice creating an AWS account (including email/phone/payment verification). Set up a Virtual MFA Device for the Root User for security 09/02/2025 09/02/2025 4 - Study AWS management tools (Console, Root/IAM User, CLI, SDK). Practice creating an IAM Admin Group and an IAM Admin User 09/03/2025 09/03/2025 5 - Research discounted payment options (On-Demand, RI, Saving Plans, Spot Instances) and the Serverless model. Practice creating an AWS Budget using a Template (monthly) 09/04/2025 09/04/2025 6 - Practice creating a Custom Cost Budget and a Usage Budget (e.g., limiting EC2 hours). Research AWS Support packages (Basic, Developer, Business, Enterprise). Clean up resources (Clean Up Budgets) 09/05/2025 09/05/2025 Week 1 Achievements: Understood the Cloud definition: On-demand delivery of IT resources over the Internet with pay-as-you-go pricing. Mastered core benefits, including cost optimization and the ability to elastically scale resources. Understood Global Infrastructure: Availability Zones (AZs) are key for fault isolation, and deploying across a minimum of 2 AZs is recommended for high availability. Completed account creation and security setup: MFA was successfully configured for the Root User. Created the IAM Admin Group and User, adhering to the security best practice of restricting Root User access. Familiarized with cost models: On-Demand (highest cost), Long-term commitment (RI/Saving Plans), and Temporary/Spare capacity (Spot Instances, up to 90% discount). Successfully set up AWS Budgets to monitor costs (Cost Budget) and track resource consumption limits (Usage Budget). Differentiated the 4 main AWS Support tiers (Basic free tier, Developer, Business, Enterprise) "},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.1-event1/","title":"AWS First Cloud Journey Community Day","tags":[],"description":"","content":"Summary Report: “AWS First Cloud Journey Community Day” Event Objectives Foster the AWS community vision and connect industry professionals. Demonstrate practical applications of Generative AI in Enterprise, Banking, and Academic sectors. Explore advanced architectures including RAG (Retrieval-Augmented Generation) and Multi-Agent Systems. Showcase serverless implementations for scalable AI solutions. Speakers Nguyễn Gia Hưng – Head of Solutions Architect, AWS Phạm Tiến Thuận Phát, Lê Minh Nghĩa, Trần Đoàn Công Lý – Enterprise Software Đinh Lê Hoàng Anh, Nguyễn Tài Minh Huy – Academic Sector Kiệt Lâm, Nguyễn Ngọc Quỳnh Mai – Banking / Internal IT Lê Phạm Ngọc Uyển, Phan Thị Thanh Thảo, Hồ Điền Đăng Khoa, Nguyễn Quang Nhật Linh – Banking / Process Automation Việt Lý – AWS Partner / Cloud \u0026amp; AI Key Highlights 1. Community Vision Opening Remarks: Mr. Nguyễn Gia Hưng kicked off the event at Bitexco Tower, outlining the vision for the AWS community and the objectives of sharing practical, hands-on cloud knowledge. 2. Enterprise \u0026amp; Contextual AI Enterprise Chatbot with MCP: The team presented on unlocking context using the Model Context Protocol (MCP) on AWS. They demonstrated how to build chatbots that handle complex enterprise context effectively, sharing lessons learned from actual implementation. 3. Real-World GenAI Applications Kitchen Recipe Recommendation: An academic showcase featuring a personalized system powered by GenAI. The session detailed the AWS workflow design used to tailor recipes to user preferences. Internal Chatbot with RAG: A deep dive into building an FAQ and knowledge base bot using Retrieval-Augmented Generation (RAG). The key takeaway was the fully serverless architecture, ensuring cost-effectiveness and scalability. 4. Advanced Automation in Banking Multi-Agent Systems: A highlight session on applying GenAI Multi-Agent Systems to automate complex banking processes. The speakers shared real-world case studies demonstrating how multiple agents coordinate to handle specific banking tasks using serverless workflows. 5. Tools \u0026amp; Orchestration GenAI with Kiro IDE \u0026amp; Strands Agent: Explored the application of GenAI in both Production and R\u0026amp;D environments. The session focused on workflow orchestration and featured a live multi-agent demo on AWS. Key Takeaways Architectural Shift From Chatbots to Agents: The industry is moving beyond simple Q\u0026amp;A bots towards Multi-Agent Systems that can execute complex tasks and workflows, especially in regulated sectors like Banking. Serverless is Key: Both RAG and Agent systems were heavily demonstrated on Serverless AWS infrastructure, highlighting it as the standard for modern AI deployment. Practical Implementation Context is King: For enterprise adoption, handling context (via MCP or RAG) is crucial for relevance and accuracy. Orchestration: Tools like Kiro IDE and Strands Agent are emerging to help manage the complexity of AI workflows. Applying to Work Evaluate Multi-Agent Systems: Investigate using a multi-agent approach for complex internal automation tasks rather than a single monolithic model. Adopt Serverless RAG: Review current internal knowledge bases and prototype a Serverless RAG solution to improve information retrieval. Explore MCP: Research the Model Context Protocol to see if it improves context retention in our current chatbot applications. Event Experience The event at Bitexco Tower provided a dense morning of technical insights. The parallel sessions allowed for a deep dive into specific industry verticals.\nNetworking: The \u0026ldquo;Welcome Coffee\u0026rdquo; and \u0026ldquo;Buffer\u0026rdquo; sessions provided excellent opportunities to connect with AWS Solution Architects and industry peers. Practical Demos: Seeing live demos of Multi-Agent systems in banking and Recipe Recommendations bridged the gap between GenAI theory and tangible product delivery. Some event photos Add your event photos here\n"},{"uri":"https://datngo196.github.io/Internship_Report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Tự động xoay vòng OIDC client secret với Application Load Balancer *Bởi Kani Murugan, đăng ngày 16 tháng 9 năm 2025, thuộc các chuyên mục: Advanced 300, Security, Identity, \u0026amp; Compliance ,Technical How-to.\nElastic Load Balancing đơn giản hóa việc xác thực bằng cách chuyển giao công việc này cho các nhà cung cấp danh tính (IdP) tương thích với OpenID Connect (OIDC). Điều này cho phép các nhà phát triển tập trung vào logic ứng dụng trong khi vẫn sử dụng cơ chế quản lý danh tính mạnh mẽ.\nOIDC client secret là các thông tin đăng nhập bí mật được sử dụng trong các giao thức OAuth 2.0 và OIDC để xác thực client (ứng dụng). Tuy nhiên, việc quản lý thủ công OIDC client secret có thể tạo rủi ro bảo mật và gia tăng gánh nặng vận hành.\nNhư hình 1 minh họa, quản lý thủ công OIDC client secret bắt đầu bằng xác thực thông qua một IdP bên thứ ba.\nHình 1: Quản lý thủ công OIDC client secret\nCác rủi ro khi quản lý thủ công OIDC client secret bao gồm:\nLộ thông tin đăng nhập dạng plaintext\nCần can thiệp thủ công để điều chỉnh cấu hình Application Load Balancer (ALB) Thiếu giám sát chủ động đối với thay đổi của thông tin đăng nhập\nThiếu xác minh liên tục các thông tin xác thực\nKhông khả thi khi mở rộng cấu hình ALB với nhiều listener rules \\\nTrong bài viết này, tôi sẽ hướng dẫn cách tự động xoay vòng OIDC client secret bằng AWS Secrets Manager, AWS Lambda và AmazonEventBridge, giúp nâng cao bảo mật và tối ưu hóa vận hành. Tự động xoay vòng secret là một thực hành bảo mật quan trọng, giúp giảm thiểu rủi ro lộ thông tin đăng nhập và hỗ trợ tuân thủ liên tục.\nĐối với cấu hình ALB-OIDC authentication, xem hướng dẫn Authenticate, Users using an Application Load Balancer.\nTổng quan giải pháp Giải pháp này cung cấp khung linh hoạt để quản lý thông tin đăng nhập tự động trên nhiều nhà cung cấp OIDC (Auth0 làm ví dụ), với một triển khai cụ thể tích hợp với các dịch vụ AWS. Kiến trúc cốt lõi hỗ trợ: xoay vòng thông tin đăng nhập tự động, lưu trữ bí mật an toàn, thiết kế không phụ thuộc vào nhà cung cấp (provider-agnostic), triển khai mở rộng cho các workflow xác thực khác nhau. Các thành phần chính bao gồm:\nSecrets Manager: Lưu trữ và quản lý an toàn thông tin đăng nhập OIDC (Auth0).\nLambda: Thực thi logic xoay vòng secret theo lịch định sẵn.\nElastic Load Balancing: Chuyển giao việc xác thực bằng các OIDC listener rules.\nEventBridge (scheduled): Kích hoạt Lambda theo lịch đã định.\nCustom AWS CloudFormation resource: Tự động hóa toàn bộ stack và kiến trúc được sử dụng trong bài viết này.\nHình 2: Tự động xoay vòng OIDC client secret\nWorkflow xác thực, như minh họa trong Hình 2, bao gồm các bước:\nEventBridge kích hoạt Lambda handler** Auth0CredentialHandler **mỗi 15 phút.\nLambda handler ``uth0CredentialHandler** **kết nối đến domain quản lý Auth0 và lấy thông tin client hiện tại — auth0_current.\nLambda handler ``Auth0CredentialHandler** truy xuất thông tin đăng nhập hiện có **auth0/credentials/${Auth0-dev-domain} từ Secrets Manager và so sánh với thông tin ``auth0_current đã lấy ở bước trước.\nNếu secret không tìm thấy, handler sẽ thử lại 3 lần trong vòng 30 phút và sau đó ghi log cảnh báo vào AWS CloudWatch.\nGiả định rằng ARN của secret đã tồn tại trong Secrets Manager\nNếu thông tin đăng nhập khác nhau, Auth0CredentialHandler **cập nhật auth0/credentials/${Auth0-dev-domain} **với giá trị mới. Nếu thông tin đăng nhập giống nhau, không thực hiện hành động nào. CloudWatch alarms được cấu hình để kích hoạt khi cập nhật secret thành công hoặc thất bại.\nALB listener rule được cấu hình để lấy thông tin client credentials một cách động từ ARN của resource auth0/credentials/${Auth0-dev-domain}** **trong Secrets Manager.\nKhuyến nghị bảo mật\nCó một số biện pháp để nâng cao bảo mật hệ thống xác thực, bao gồm: triển khai quản lý secret tập trung với mã hóa dữ liệu khi lưu trữ (encryption at rest), cấu hình Lambda với quyền tối thiểu (least-privilege), chỉ giới hạn truy cập đến các resource cần thiết trong Secrets Manager và ALB listener, giúp giảm phạm vi rủi ro bảo mật (security blast radius).\nSử dụng CloudWatch alarms để giám sát các sự kiện quan trọng, bao gồm Cập nhật secret, Thất bại khi cập nhật secret, Các vấn đề liên quan đến credential của ALB và Sử dụng AWS Config để theo dõi cấu hình rule và thực hiện đánh giá bảo mật định kỳ.\nBằng cách Tạo secret riêng biệt cho từng ALB listener rule, cho phép kiểm soát truy cập chi tiết (granular access control) và thu hẹp phạm vi quyền hạn, giúp nâng cao bảo mật tổng thể của hệ thống.\nBằng cách tuân theo các thực hành này, bạn có thể thiết lập khung bảo mật vững chắc cho ứng dụng, đồng thời đảm bảo bảo vệ dữ liệu và quản lý truy cập đúng cách.\nYêu cầu tiên quyết\nGiải pháp này giả định rằng các điều kiện sau đã được đáp ứng trước khi triển khai:\nMột ALB hiện có được cấu hình với listener và target group, sử dụng làm Listenerarn và targetarn trong CloudFormation template.\nTài khoản OIDC IdP (ví dụ: Auth0) và ứng dụng client.\nThông tin đăng nhập client của ứng dụng Auth0 IdP đã được lưu trữ trong Secrets Manager.\n{ \u0026#34;domain\u0026#34;: \u0026#34;your-tenant.auth0.com\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;your-client-id\u0026#34;, \u0026#34;client_secret\u0026#34;: \u0026#34;your-client-secret\u0026#34; } Chi tiết triển khai\nLưu ý: Giải pháp này minh họa tự động xoay vòng OIDC client secret sử dụng Auth0 làm IdP. Mặc dù các nguyên tắc cốt lõi và mô hình kiến trúc có thể áp dụng rộng rãi, các chi tiết triển khai cụ thể có thể khác nhau tùy từng nhà cung cấp danh tính. Người dùng nên tham khảo tài liệu của IdP cụ thể để biết các bước cấu hình chính xác, cách tương tác API và các cơ chế xác thực tương thích với AWS.\nĐây là một phương pháp tự động, đơn giản và có thể mở rộng, sử dụng CloudFormation custom resource để tạo các resource được minh họa trong sơ đồ kiến trúc. CloudFormation template và AWS Lambda implementation được lưu trữ trong demo-stack.\nCác thành phần cốt lõi\nTrong phần này, tôi sẽ giải thích các thành phần chính của giải pháp.\nQuy tắc làm mới thông tin xác thực\nMột EventBridge rule được lên lịch để kích hoạt Lambda function Auth0CredentialHandler** **mỗi 15 phút, sử dụng LambdaInvokePermission của AWS Identity and Access Management (IAM) role.\nAuth0CredentialHandler Lambda function\nLambda function ``Auth0CredentialHandler chịu trách nhiệm quản lý client credentials một cách an toàn. Nó truy xuất cấu hình Auth0 từ Secrets Manager tại resource auth0/credentials/${Auth0-dev-domain}, thực hiện API call đến domain Auth0 để lấy token mới, quản lý cập nhật thông tin đăng nhập mới vào Secrets Manager. Lambda này cần quyền truy cập Secrets Manager, được cấp thông qua execution role.\nIAM role của Lambda có hai bộ quyền chính:\nAWS managed policy ``AWSLambdaBasicExecutionRole, cho phép Lambda tạo log trên CloudWatch. \\\nCustom policy, cấp quyền cụ thể trên Secrets Manager (``GetSecretValue``, ``CreateSecret``, ``UpdateSecret``) cho các secret dưới path auth0/credentials/${Auth0-dev-domain}.\nLambda sẽ thử lại 3 lần trong vòng 30 phút. Nếu tất cả các lần thử thất bại, CloudWatch sẽ ghi cảnh báo và tạo alarms.\nManagedPolicyArns: - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole Policies: - PolicyName: SecretsManagerAccess PolicyDocument: Version: \u0026#39;2012-10-17\u0026#39; Statement: - Effect: Allow Action: - secretsmanager:GetSecretValue - secretsmanager:CreateSecret - secretsmanager:UpdateSecret Resource: - !Sub arn:aws:secretsmanager:${AWS::Region}:${AWS::AccountId}:secret:auth0/* # Permission for Amazon EventBridge to invoke Lambda LambdaInvokePermission: Type: AWS::Lambda::Permission Properties: Action: lambda:InvokeFunction FunctionName: !Ref Auth0CredentialHandler Principal: events.amazonaws.com SourceArn: !GetAtt CredentialRefreshRule.Arn ALB Listener Rules\nCác resource listener rule của Elastic Load Balancing trong CloudFormation được cấu hình để lấy động thông tin client credentials từ Secrets Manager và chuyển tiếp các request đã xác thực đến target group cụ thể. Chúng tích hợp với thông tin đăng nhập Auth0, được Lambda** Auth0CredentialHandler **tự động làm mới định kỳ. Cấu hình này yêu cầu quyền đọc (read access) trên Secrets Manager để lấy Auth0 client credentials phục vụ quá trình xác thực.\n# ALB Listener Rules - replace the Oidc config with your endpoints. Only Client credentials are stored in SecretsManager ListenerRule1: Type: AWS::ElasticLoadBalancingV2::ListenerRule Properties: ListenerArn: arn:aws:elasticloadbalancing:region:account-id:listener/app/my-load-balancer/1234567890/abcdef Priority: 1 Actions: - Type: authenticate-oidc AuthenticateOidcConfig: ClientId: \u0026#39;{{resolve:secretsmanager:auth0/credentials/your-tenant.auth0.com:SecretString:client_id}}\u0026#39; ClientSecret: \u0026#39;{{resolve:secretsmanager:auth0/credentials/your-tenant.auth0.com:SecretString:client_secret}}\u0026#39; Issuer: https://idp1.example.com AuthorizationEndpoint: https://idp1.example.com/auth TokenEndpoint: https://idp1.example.com/token UserInfoEndpoint: https://idp1.example.com/userinfo OnUnauthenticatedRequest: authenticate - Type: forward TargetGroupArn: arn:aws:elasticloadbalancing:region:account-id:targetgroup/target-group-1/1234567890abc Conditions: - Field: path-pattern Values: - /app1/* Giám sát và cảnh báo với CloudWatch\nCloudFormation template được cung cấp được cấu hình để thiết lập giám sát bảo mật cho các cập nhật secret. Template này thực hiện Cấu hình cảnh báo cho cả cập nhật secret thành công và cập nhật thất bại, Tạo các CloudWatch metric filter sử dụng AWS CloudTrail logs, Thiết lập các alarm tương ứng với các ngưỡng đã định, Tạo một Amazon Simple Notification Service (Amazon SNS) topic để gửi cảnh báo tổng hợp. Khi triển khai, giải pháp hạ tầng như mã (infrastructure-as-code) này sẽ tự động phát hiện và thông báo các sự kiện bảo mật tiềm ẩn liên quan đến quản lý secret và các nỗ lực truy cập trái phép.\n# CloudWatch Log Group CloudTrailLogGroup: Type: AWS::Logs::LogGroup Properties: LogGroupName: secrets-manager-monitoring RetentionInDays: 14 # Combined Metric Filter for Both Success and Failed Updates SecretUpdateMetricFilter: Type: AWS::Logs::MetricFilter Properties: LogGroupName: !Ref CloudTrailLogGroup FilterPattern: !Sub \u0026#39;{ $.eventSource = secretsmanager.amazonaws.com \u0026amp;\u0026amp; ($.eventName = UpdateSecret || $.eventName = PutSecretValue) \u0026amp;\u0026amp; $.responseElements.ARN = \u0026#34;${MyCustomResource.SecretArn}\u0026#34; }\u0026#39; MetricTransformations: - MetricNamespace: \u0026#39;SecretsManager/Updates\u0026#39; MetricName: \u0026#39;SecretUpdates\u0026#39; MetricValue: \u0026#39;1\u0026#39; DefaultValue: 0 # Combined Alarm for Both Success and Failed Updates SecretUpdateAlarm: Type: AWS::CloudWatch::Alarm Properties: AlarmName: !Sub \u0026#39;${AWS::StackName}-secret-update\u0026#39; AlarmDescription: !Sub \u0026#39;Alarm for any updates (success or failure) to secret ${MyCustomResource.SecretArn}\u0026#39; MetricName: SecretUpdates Namespace: SecretsManager/Updates Statistic: Sum Period: 300 EvaluationPeriods: 1 Threshold: 0 ComparisonOperator: GreaterThanThreshold TreatMissingData: notBreaching AlarmActions: - !Ref SecretMonitoringTopic Để nâng cao độ tin cậy của quá trình xoay vòng secret, hãy triển khai giám sát toàn diện bằng cách: Tạo CloudWatch alarms để phát hiện thất bại khi Lambda thực hiện xoay vòng vượt quá ngưỡng và tỷ lệ lỗi xác thực cao, Giám sát các đột biến bất thường trong tỷ lệ lỗi HTTP 4xx và 5xx từ ALB, Sử dụng CloudTrail để theo dõi các API call và thay đổi cấu hình liên quan đến secrets trong Secrets Manager và các thiết lập load balancer. Bằng cách triển khai các alarm tùy chỉnh này cùng với các cấu hình mặc định, các sự cố bảo mật tiềm ẩn và nỗ lực truy cập trái phép có thể được phát hiện nhanh chóng trên toàn bộ tài nguyên AWS của bạn. Phương pháp nhiều lớp này giúp duy trì khả năng giám sát quá trình xoay vòng secret và nhanh chóng xác định, phản ứng với các vấn đề tiềm ẩn.\nXem hướng dẫn chi tiết tại Creating CloudWatch alarms for CloudTrail events: examples.\nQuy trình triển khai\nTriển khai CloudFormation template bằng AWS Command Line Interface (AWS CLI) hoặc AWS Management Console. Thay \u0026lt;your-region\u0026gt; bằng AWS Region mà bạn muốn triển khai giải pháp.\naws cloudformation deploy \\ --template-file template.yaml \\ --stack-name oidc-credential-manager-stack \\ --capabilities CAPABILITY_IAM \\ --region Lưu ý: Bạn có thể thêm các tham số bổ sung nếu được yêu cầu bởi cấu hình IdP của bạn.\nKiểm thử và xác minh\nTuyên bố: Khuyến nghị thử nghiệm trong môi trường riêng biệt, không quan trọng để đảm bảo mọi cài đặt cụ thể của khách hàng được xác minh đầy đủ trước khi triển khai vào môi trường sản xuất.\nĐối với cập nhật secret, hãy xác minh rằng các CloudWatch alarms đã được cấu hình được kích hoạt. Đối với xác thực ALB, kiểm tra ALB access logs để tìm các entry authentication_success và sự hiện diện của OIDC identity tokens.\nThiết lập CloudWatch metrics và alarms để giám sát quá trình xoay vòng secret và tỷ lệ xác thực thành công. Xác minh các trường hợp thất bại bằng cách sửa thủ công cấu hình ALB rule trỏ tới một secret ARN khác và xác nhận rằng CloudWatch alarm được kích hoạt. Dưới đây là một ví dụ về sự kiện CloudTrail cho một cập nhật Secrets Manager thành công.\n{ \u0026#34;source\u0026#34;: [\u0026#34;aws.secretsmanager\u0026#34;], \u0026#34;detail-type\u0026#34;: [\u0026#34;AWS API Call via CloudTrail\u0026#34;], \u0026#34;detail\u0026#34;: { \u0026#34;eventSource\u0026#34;: [\u0026#34;secretsmanager.amazonaws.com\u0026#34;], \u0026#34;eventName\u0026#34;: [\u0026#34;UpdateSecret\u0026#34;], \u0026#34;responseElements\u0026#34;: {\u0026#34;status\u0026#34;: \u0026#34;Success\u0026#34;} } } Sau đây là một ví dụ về nhật ký truy cập ALB:\n/aws/alb/\u0026lt;your-alb-name\u0026gt;: - Look for entries containing: \u0026#34;authentication_success\u0026#34; \u0026#34;id_token_authentication_successful\u0026#34; \u0026#34;x-amzn-oidc-identity\u0026#34; HTTP status code 200 - Example log pattern: timestamp elb_name client:port target:port request_processing_time target_processing_time response_processing_time status_code \u0026#34;authentication_success\u0026#34; \u0026#34;x-amzn-oidc-identity: [token]\u0026#34; Kịch bản nâng cao\nTrong phần này, bạn sẽ học cách giảm thời gian chờ và làm cho cập nhật Secrets Manager gần như đồng bộ (synchronous).\nTối ưu hóa đồng bộ Secrets Manager: Sử dụng EventBridge partner tích hợp để cấu hình EventBridge gọi Lambda function dựa trên các sự kiện nhận được từ IdP bên thứ ba. Xem hướng dẫn chi tiết tại Receiving events from a SaaS partner with Amazon EventBridge . \\\nXoay vòng client ID: Trong khi xoay vòng client secret là kịch bản phổ biến nhất, đôi khi cũng cần xoay vòng client ID. Trong hầu hết các IdP, điều này có nghĩa là tạo một client ứng dụng mới và di chuyển các resource. Để thực hiện, Lambda Auth0CredentialHandler cần quyền sửa đổi ALB listener rules (elasticloadbalancing``:``ModifyRule``, ``elasticloadbalancing``:DescribeListeners, ``elasticloadbalancing``:DescribeRules). Xoay vòng client ID có thể gây gián đoạn xác thực tạm thời, vì vậy thử nghiệm kỹ lưỡng là cần thiết. Sử dụng AWS Config để giám sát cấu hình ALB rule nhằm phát hiện các thay đổi bất ngờ. Tính năng này tăng cường bảo mật tổng thể, mặc dù có thể tăng độ phức tạp của giải pháp và đôi khi cần can thiệp thủ công. \\\nChiến lược đa nhà cung cấp (multi-provider): Nếu tổ chức của bạn quản lý nhiều IdP, hãy triển khai framework xoay vòng tập trung để trừu tượng hóa các khác biệt riêng của nhà cung cấp, tập trung vào các nguyên tắc bảo mật cốt lõi được nêu trong bài viết này. Các cân nhắc chính bao gồm: tạo giao diện độc lập với nhà cung cấp để hỗ trợ giám sát toàn diện và giảm thiểu chi phí cấu hình.\nKết luận\nTrong bài viết này, bạn đã khám phá cách tiếp cận toàn diện để tự động xoay vòng OIDC client secret sử dụng các dịch vụ của AWS. Bằng cách triển khai giải pháp này, bạn có thể: Nâng cao bảo mật ứng dụng, Giảm thiểu chi phí quản lý thủ công, Duy trì chiến lược xác thực vững chắc. Hãy cân nhắc khám phá các kỹ thuật quản lý danh tính nâng cao hoặc tích hợp xác thực đa yếu tố (MFA) với triển khai OIDC của bạn. Nếu bạn mới làm quen với tự động xoay vòng secrets, tham khảo bài viết Back to Basics: Secrets Management.\nKani Murugan là một kỹ sư bảo mật đã được bổ nhiệm biên chế (tenured) tại Amazon Security, nơi cô chuyên về bảo mật sản phẩm, tập trung vào bảo mật ứng dụng, mạng và dữ liệu. Với hơn 8 năm kinh nghiệm trong nhiều lĩnh vực bảo mật khác nhau, Kani mang đến cho công việc của mình một nền tảng kiến thức phong phú. Ngoài công việc, Kani là người đam mê anime và là một độc giả “không kén chọn”, đọc nhiều chủ đề đa dạng. "},{"uri":"https://datngo196.github.io/Internship_Report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"MAPVIBE - AI-Powered Map Location Discovery Platform (Discover dining and other locations in Ho Chi Minh City using natural-language prompts and contextual insights)\n1. Executive Summary MapVibe is an AI-driven web platform launched in Ho Chi Minh City to transform location discovery, enabling users to find venues through natural-language prompts (e.g., “find a luxury rooftop restaurant with city view open until midnight” or “quiet coffee shop near the river with outdoor seating”). The platform harnesses Amazon Bedrock’s Large Language Models (LLMs) to interpret user intent, integrating real-time contextual factors like location, time, and preferences, and retrieves data from an internal DynamoDB database. Built on a serverless AWS architecture, MapVibe delivers low latency (\u0026lt;10s), high accuracy (≥85% match satisfaction), and cost efficiency (\u0026lt;$200 for initial 8-week development and demo cycle, completed by October 22, 2025). Authenticated users enjoy personalized recommendations, the ability to contribute reviews, and access to moderation tools, all enhanced by AI technologies.\n2. Problem Statement What’s the Problem? Traditional map platforms like Google Maps rely on keyword-based searches and static filters, struggling to interpret nuanced, context-rich queries (e.g., “quiet coffee shop near the river with outdoor seating”). Users waste time navigating multiple apps to find suitable dining or activity locations. Existing solutions lack conversational interfaces and fail to incorporate contextual signals like time, mood, or group size. The Solution MapVibe employs AWS Bedrock LLMs to parse natural-language prompts in Vietnamese and English, converting them into structured queries. It retrieves and ranks results from an internal DynamoDB database with geo-indexed place data, offering a hybrid interface (conversational search + category filters). User-generated content (reviews, place suggestions) is moderated using AWS Rekognition, ensuring safety and quality through advanced AI-driven analysis.\nBenefits and ROI Speed: Reduces location discovery time from minutes to seconds. Personalization: Context-aware results based on user preferences and behavior, powered by AI. Automation: Eliminates manual filtering with AI-driven intent parsing. Scalability: Global AWS infrastructure ensures low latency and resilience. Cost Efficiency: Optimized to fit within a $200 budget for the initial 8-week cycle, completed by October 22, 2025. Commercial Potential: Opportunities for partnerships with local businesses or integration with internal booking systems. 3. Solution Architecture Overview User Prompt + Context → Bedrock LLM Intent Parsing → Structured Query → DynamoDB Search → Rank \u0026amp; Cache → Web UI Display → User Feedback Loop.\nAWS Services Used Service Function Amazon Route 53 Domain routing AWS Certificate Manager SSL/TLS certificates AWS WAF Web application firewall Amazon CloudFront Global CDN for static assets Amazon API Gateway Secure RESTful API endpoints AWS Lambda Intent parsing, search, and ranking logic Amazon DynamoDB Geo-indexed place data and query caching Amazon S3 Storage for photos, logs, and assets Amazon Cognito User authentication and authorization Amazon Bedrock LLM for intent parsing and summarization Amazon Rekognition AI-driven content moderation for user uploads Amazon EventBridge Scheduled analytics and badge updates Amazon CloudWatch Monitoring and logging Component Design Frontend: Responsive web app (Next.js, bilingual VI/EN, hybrid search UI). Data Ingestion: Prompts and context processed via API Gateway; user uploads (reviews, photos) moderated by Rekognition’s AI. Data Storage: DynamoDB for place data and cached queries (24-hour TTL); S3 for photos and logs. Data Processing: Lambda microservices handle Bedrock LLM calls, query execution, and result ranking. User Management: Cognito for JWT-based authentication (email/social login); guest users access limited features. Output: Displays place cards with AI-generated summaries, ratings, photos, and CTAs (e.g., Get Directions, Call). 4. Technical Implementation Implementation Phases Phase Description Duration 1 Define architecture, Bedrock prompt schema, and DynamoDB schema 2 weeks 2 Estimate costs and optimize caching strategy 1 week 3 Build backend (Lambda, DynamoDB, Bedrock, Rekognition) 3 weeks 4 Develop frontend (Next.js, bilingual, responsive UI) 3 weeks 5 Test and optimize for \u0026lt;10s latency and scalability 2 weeks 6 Launch MVP, deploy via CI/CD, collect feedback 2 weeks Technical Requirements Edge Devices: Modern browsers (Chrome, Safari, Firefox) with PWA-ready responsive UI. Cloud: AWS Route 53, ACM, WAF, CloudFront, API Gateway, Lambda, DynamoDB, S3, Cognito, Bedrock, Rekognition, EventBridge, CloudWatch. Tools \u0026amp; Frameworks: Next.js (App Router), TypeScript, AWS CDK for infrastructure-as-code, GitHub Actions for CI/CD. 5. Timeline \u0026amp; Milestones Period Activities Pre-Development (Month 0 - Sept 2025) Research Ho Chi Minh City venue datasets for DynamoDB Month 1 (Oct 2025) Build backend MVP with Bedrock LLM and DynamoDB Month 2 (Nov 2025) Implement caching, develop frontend integration Month 3 (Nov 2025) Launch public beta, optimize performance, collect feedback Post-Launch (Dec 2025) Add advanced features (e.g., ML-based ranking, offline mode) 6. Budget Estimation Cloud Infrastructure Costs AWS Service Cost/Month (USD) Description Lambda 15 API + LLM logic DynamoDB 10 Cached query store S3 5 Logs, static files API Gateway 10 Request routing Cognito 5 Auth MAU CloudFront 10 Hosting/CDN Bedrock (LLM tokens) 15 Prompt parsing Rekognition 5 Batch image moderation CloudWatch 5 Error-only logging Total ≈ 80/month ≈ 160/8 weeks Cost Optimization Measures Free-Tier Utilization: Leverage AWS free tiers for Lambda, DynamoDB, S3, CloudFront, Rekognition, and Cognito to minimize costs. Aggressive Caching for Bedrock: Achieve a 95% cache hit rate to reduce AI token costs from $120 to \u0026lt;$15/month. Batch Rekognition Processing: Non-real-time image checks save ~$80 over 8 weeks. Simplified Load Testing: 100 users × 10 min scenario instead of 300 × 30 min reduces compute costs. Reduced CloudWatch Logging: Error-only logs save $50+ over 8 weeks. No Provisioned Concurrency: Avoids idle Lambda costs. Environment Variables: Use instead of Secrets Manager to eliminate secret storage charges. On-Demand DynamoDB Mode: All reads/writes free under tier. Disabled Origin Shield: Saves CloudFront overhead. Static Asset Caching: Minimizes outbound data transfer costs. Recommended Budget Scenarios To ensure the MapVibe platform operates efficiently within the $200 AWS budget over the initial 8-week development and demo cycle (completed by October 22, 2025), we recommend the following scenarios based on varying levels of optimization and resource usage:\nMinimal Scenario: Focuses on essential features with maximum reliance on free tiers. This includes disabling non-critical services like WAF if not needed, limiting Bedrock invocations to cached queries only (targeting 98%+ cache hit rate), and conducting no load testing. Estimated cost: \u0026lt;$50 over 8 weeks. Suitable for initial prototyping but may compromise demo reliability due to potential untested scalability issues.\nRecommended Scenario: Balances cost and reliability by incorporating all key optimization measures listed above. This scenario utilizes aggressive caching (95% hit rate for Bedrock), batch processing for Rekognition, simplified load testing (100 users × 10 min), and error-only logging in CloudWatch. It ensures low latency and resilience while staying well under budget. Estimated cost: ~$100-150 over 8 weeks. Ideal for the MVP demo launched on October 22, 2025, providing a robust experience without unnecessary expenses.\nEnhanced Scenario: Includes additional provisions for higher usage post-launch, such as provisioned concurrency for Lambda during peak times and full logging in CloudWatch for detailed debugging. This increases costs slightly but enhances performance monitoring and scalability testing (e.g., 300 users × 30 min loads). Estimated cost: ~$180-200 over 8 weeks. Recommended for ongoing operations after October 22, 2025, if extended demos or higher traffic is anticipated, still within the overall budget cap.\nRecommendation: The Recommended Scenario was successfully implemented for the MVP launch on October 22, 2025, ensuring optimal demo reliability, scalability, and cost control within the $200 budget. For ongoing operations, consider transitioning to the Enhanced Scenario as needed.\nCost Control \u0026amp; Monitoring Create billing alerts and enable AWS Cost Explorer. Tag all resources (Project=MapVibe, Environment=Dev). Review weekly: CloudFront data \u0026gt; 50 GB/week Bedrock cache hit \u0026lt; 90% Lambda invocations \u0026gt; 100K/week Total cost \u0026gt; $15/week 7. Risk Assessment Risk Impact Probability Mitigation DynamoDB data inconsistency High Medium Regular data validation and backups Inaccurate LLM parsing (VN/EN) Medium Low Predefined prompt templates, validation Scalability under high load Medium Medium Serverless auto-scaling, caching Privacy concerns (location data) High Low Explicit user consent, anonymized queries Contingency Plans: Use cached DynamoDB results or local JSON fallback for demos. Implement IP-based rate limits for unauthenticated users.\n8. Expected Outcomes Technical Improvements Conversational Search: Natural-language support for Vietnamese and English with \u0026lt;10s latency, powered by Bedrock LLMs. AI Summaries: Bedrock-generated place overviews, refreshed every 7 days or after 10 new reviews. Scalability: Serverless architecture with global CDN delivery via CloudFront. Moderation: Rekognition’s AI ensures safe user-generated content (reviews, photos). Long-Term Value Personalization: ML-based re-ranking and user behavior analysis. Offline Support: PWA for offline shortlisting of venues. Extensibility: Potential integration with internal booking systems. Contextual Expansion: Recommendations based on weather, events, or social trends. Attachments / References AWS Pricing Calculator GitHub Repository 9. IMPORTANT: Read the SRS to know more about our project Software Requirement Specification Related Documents "},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.2-event2/","title":"AI-Driven Development Workshop","tags":[],"description":"","content":"Summary Report: “AI-Driven Development Workshop” Event Objectives Understand the overview of the AI-Driven Software Development Life Cycle (SDLC) Access and view a real-world demo of Amazon Q Developer Deep dive into the Kiro tool and its application in the development process Update on the latest developer support tools to increase productivity Speakers Toan Huynh – Speaker on AI-Driven SDLC \u0026amp; Amazon Q My Nguyen – Speaker on Kiro Demonstration Key Highlights AI-Driven Development Life Cycle (SDLC) Shift in Paradigm: Shifting from traditional development processes to processes with integrated AI support at every stage. Automation: Automating repetitive tasks in the development lifecycle, from coding to testing and deploying. Efficiency: Optimizing time and resources thanks to intelligent AI assistants. Amazon Q Developer Demonstration Coding Assistant: Demo of code suggestions, code logic explanation, and automatic unit test generation. Troubleshooting: Using Amazon Q to debug and find solutions for technical errors in real-time. Integration: How to integrate Amazon Q into the development environment (IDE) and daily workflows. Kiro Demonstration Tool Capabilities: Introduction to the core features of Kiro (related to Kiro IDE/Agent). Practical Use Cases: Live demo on how to use Kiro to solve specific programming problems. Developer Experience: Improving the developer experience through Kiro\u0026rsquo;s interface and intelligent features. Key Takeaways Tooling Landscape Amazon Q Developer is not just a chat tool but a comprehensive assistant for the SDLC. Kiro brings new approaches to supporting the development environment (IDE/Agent). Productivity Focus Applying AI-Driven SDLC helps minimize manual tasks, allowing developers to focus on complex business logic. It is necessary to proactively get used to these tools to avoid falling behind in new technology trends. Applying to Work Integrate Amazon Q: Install and pilot Amazon Q Developer in current projects to support code reviews and test writing. Explore Kiro: Dedicate time to research Kiro more deeply after the demo to consider its applicability to the team\u0026rsquo;s workflow. Review SDLC Process: Re-evaluate the team\u0026rsquo;s current development process and identify bottlenecks that can be solved with AI. Event Experience The afternoon workshop focused deeply on technical demos, providing an intuitive view of the power of modern programming support tools.\nHands-on Insight Toan Huynh\u0026rsquo;s presentation helped me clearly visualize what an \u0026ldquo;AI-ized\u0026rdquo; SDLC process looks like. My Nguyen\u0026rsquo;s demo on Kiro was very practical, showing the potential of emerging tools alongside giants like AWS. Impact on Workflow It became clear that AI is changing the way we write software: faster, more accurate, and with fewer errors. The event was concise but substantial, going straight into the combat tools that developers care about. Some event photos Add your event photos here\nOverall, the afternoon session was the perfect complement in terms of Tools to the Architecture knowledge gained earlier, completing the picture of modern software development.\n"},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Delve into critical networking services on the AWS platform. Set up and configure a secure Virtual Private Cloud (VPC) environment, including the creation of Public and Private Subnets distributed across multiple Availability Zones (AZs) to ensure a high-availability architecture. Master the Internet connectivity mechanism for resources within the VPC, differentiating the roles of the Internet Gateway (IGW) and the NAT Gateway. Understand and implement AWS network security layers: Network Access Control Lists (NACLs) at the Subnet level and Security Groups (SGs) at the Elastic Network Interface (ENI) level. Research large-scale networking solutions between multiple VPCs, specifically VPC Peering and Transit Gateway. Familiarize with the types of Elastic Load Balancers (ELB), focusing on Layer 7 routing capabilities (ALB) and Layer 4 extreme performance (NLB). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Set up Basic VPC Architecture (Multi-AZ): Create a VPC with a specified CIDR range. Create at least 4 Subnets (Public/Private across 2 different AZs) to ensure a high-availability architecture. Understand the rule of 5 IP addresses reserved by AWS in each Subnet.\n09/08/2025 09/08/2025 3 - Configure Internet Gateway (IGW): Create and attach an IGW to the VPC. Create a Custom Route Table for Public Subnets. Route Internet traffic (0.0.0.0/0) via the IGW. Associate this Route Table with the Public Subnets. 09/09/2025 09/09/2025 4 - Deploy NAT Gateway: Differentiate NAT Gateway and NAT Instance. Allocate an Elastic IP. Deploy the NAT Gateway in a Public Subnet. Configure the Route Table for Private Subnets, routing Internet traffic (0.0.0.0/0) through the NAT Gateway, allowing outbound access only. 09/10/2025 09/10/2025 5 - Configure Security and Deploy EC2: Study and differentiate Security Groups (stateful, ENI level, ALLOW only) and NACLs (stateless, Subnet level, ALLOW/DENY). Create Security Groups for public and private hosts. Deploy EC2 in Public and Private Subnets. Test connectivity between Subnets and to the Internet (using a Bastion Host/Jump Host concept to SSH into Private EC2). 09/11/2025 09/11/2025 6 - Connectivity and ELB: Learn about VPC Peering (1:1 connection, no transitive routing support) and Transit Gateway (Central Hub for connecting a large number of VPCs). Explore Elastic Load Balancing (ELB), focusing on ALB (Layer 7, path-based routing) and NLB (Layer 4, extreme performance, static IP support). 09/12/2025 09/12/2025 Week 2 Achievements: Established a Complete VPC: Successfully created a VPC and divided Subnets into Public/Private tiers across multiple Availability Zones (AZs), ensuring a high-availability architecture. Mastered Internet Outbound/Inbound Mechanism: Configured IGW to allow Public Internet access for Public Subnets. Secured Internet Access for Private Subnets: Deployed a NAT Gateway (placed in a Public Subnet) and configured the corresponding Route Table, allowing instances in the Private Subnet to access the Internet outbound only. Applied Network Layer Security: Differentiated and configured Security Groups (Stateful, applied at ENI) and NACLs (Stateless, applied at Subnet). Successfully deployed EC2 Instances in both Public and Private Subnets and verified internal and external connectivity using jump hosts (Bastion Host) Researched Large-Scale Connectivity: Understood the functional difference between VPC Peering (1:1) and the scalable Transit Gateway (Hub-and-spoke). Familiarized with Elastic Load Balancer types (ALB for Layer 7 routing, NLB for Layer 4 extreme performance) and core features "},{"uri":"https://datngo196.github.io/Internship_Report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Sử dụng Raspberry Pi 5 như các nút lai (Hybrid Nodes) của Amazon EKS cho các khối tải (workload) tại biên (edge) *Bởi Alberto Crescini, Gladwin Neo, và Utkarsh Pundir, đăng ngày 17 tháng 9 năm 2025, thuộc các chuyên mục:Amazon Elastic Kubernetes Service **, Manufacturing *, Technical How-to.\nKể từ khi ra mắt, Amazon Elastic Kubernetes Service (Amazon EKS) đã vận hành hàng chục triệu cụm (cluster), giúp người dùng tăng tốc triển khai ứng dụng, tối ưu chi phí, và tận dụng tính linh hoạt của Amazon Web Services (AWS) trong việc lưu trữ và vận hành các ứng dụng container hóa. Amazon EKS loại bỏ những phức tạp trong việc duy trì hạ tầng control plane của Kubernetes, đồng thời tích hợp liền mạch với các tài nguyên và hạ tầng AWS.\nTuy nhiên, một số khối tải (workload) cần được chạy tại biên (edge) để xử lý theo thời gian thực, chẳng hạn như các ứng dụng nhạy cảm với độ trễ (latency-sensitive) hoặc tạo ra lượng dữ liệu khổng lồ cần xử lý nhanh tại chỗ.\nTrong những tình huống như vậy, khi có kết nối Internet ổn định, người dùng thường muốn duy trì lợi ích của việc tích hợp đám mây, đồng thời vẫn sử dụng phần cứng tại chỗ (on-premises). Chính vì thế, tại AWS re:Invent 2024, chúng tôi đã giới thiệu Amazon EKS Hybrid Nodes — một giải pháp cho phép người dùng mở rộng mặt dữ liệu (data plane) của Kubernetes ra biên, trong khi giữ control plane chạy trong AWS Region. Amazon EKS Hybrid Nodes giúp thống nhất việc quản lý Kubernetes trên môi trường đám mây, tại chỗ và tại biên, bằng cách cho phép người dùng sử dụng hạ tầng tại chỗ như các node trong cụm EKS, bên cạnh Amazon Elastic Compute Cloud (Amazon EC2).\nĐể minh họa cách sử dụng Amazon EKS Hybrid Nodes, chúng tôi trình bày một kịch bản thực tế trong lĩnh vực sản xuất (manufacturing) — nơi các hệ thống thường phụ thuộc vào dữ liệu thời gian thực từ các cảm biến số (digital sensors), cần được xử lý cục bộ do yêu cầu về độ trễ và độ tin cậy, trong khi vẫn tận dụng đám mây để phân tích và lưu trữ dài hạn.\nTrong ví dụ này, hệ thống đọc các giá trị khoảng cách từ cảm biến siêu âm (ultrasonic sensor), xử lý dữ liệu trực tiếp trên thiết bị biên (edge device) đang hoạt động như một Hybrid Node, và sau đó lưu trữ dữ liệu vào Amazon DynamoDB trên AWS.\nTrong bài viết này, chúng tôi sẽ hướng dẫn cách triển khai Amazon EKS Hybrid Nodes sử dụng Raspberry Pi 5, một nền tảng phổ biến cho tính toán biên (edge computing). Bài viết bao gồm:\nThiết lập cụm EKS kết nối liền mạch giữa hạ tầng đám mây và hạ tầng biên\nBảo mật kết nối bằng WireGuard VPN để thiết lập truyền thông site-to-site\nKích hoạt mạng container bằng Cilium cho các triển khai sử dụng Hybrid Nodes\nTrình bày một ứng dụng Internet of Things (IoT) thực tế nhằm minh họa sức mạnh của sự tích hợp giữa điện toán biên và điện toán đám mây (edge–cloud integration)\nVì sao chọn Raspberry Pi 5? Raspberry Pi 5 có thiết kế nhỏ gọn và có thể triển khai tại biên (edge), cho phép xử lý dữ liệu trước khi truyền lên đám mây. Tận dụng ưu điểm này, chúng tôi xây dựng một ứng dụng kiến trúc microservices, trong đó một phần chạy tại biên trên Raspberry Pi 5 và một phần chạy trên AWS trong môi trường đám mây. Ở phía biên, Raspberry Pi cục bộ được kết nối với cảm biến siêu âm (ultrasonic sensor) để thu nhận dữ liệu khoảng cách theo thời gian thực. Dữ liệu này sau đó được xử lý và tải lên cơ sở dữ liệu DynamoDB trên AWS. Tiếp theo, dữ liệu được hiển thị thông qua một bảng điều khiển (dashboard) chạy như một triển khai độc lập trong cụm (cluster deployment). Với cách triển khai này, bạn có thể tiền xử lý dữ liệu tại chỗ, từ đó giảm lượng dữ liệu cần truyền lên AWS, giúp tối ưu băng thông và chi phí, đồng thời tăng hiệu quả xử lý cục bộ cho các ứng dụng biên.\nTổng quan kiến trúc Trong môi trường đám mây, chúng tôi triển khai một Amazon Virtual Private Cloud (Amazon VPC) chứa cụm Amazon EKS. Bên trong VPC này, một phiên bản Amazon EC2 đóng vai trò cổng kết nối (gateway) giữa môi trường đám mây và mạng biên (edge network) tại cơ sở. Phiên bản EC2 này thiết lập một đường hầm VPN bảo mật site-to-site sử dụng WireGuard, kết nối với Raspberry Pi 5, thiết bị đóng vai trò là Hybrid Node của chúng tôi. Khi đường hầm VPN được thiết lập, lưu lượng dữ liệu giữa Raspberry Pi và đám mây sẽ được định tuyến thông qua máy chủ WireGuard đang chạy trên Amazon EC2, giúp mở rộng cụm EKS ra đến vùng biên. Từ góc nhìn của cụm EKS, Raspberry Pi hoạt động giống như bất kỳ node nào khác, mặc dù nó nằm ngoài phạm vi của VPC. Kiến trúc tổng thể được thể hiện trong hình minh họa bên dưới.\nMặt điều khiển Kubernetes (control plane) được quản lý hoàn toàn bởi AWS, bao gồm API server, etcd datastore, scheduler và controller manager. Trong phần hướng dẫn này, chúng tôi cấu hình control plane của Kubernetes với điểm truy cập công khai (public endpoint), cho phép các node Raspberry Pi có thể giao tiếp với control plane thông qua Internet. AWS đảm nhận toàn bộ sự phức tạp trong việc bảo mật và mở rộng control plane của Kubernetes để đảm bảo tính sẵn sàng cao (high availability), giúp bạn có thể tập trung vào phát triển và triển khai ứng dụng của mình.\nChúng tôi sử dụng một phiên bản EC2 chuyên dụng chạy WireGuard, đóng vai trò cổng VPN (VPN gateway), tạo đường hầm bảo mật giữa AWS và hạ tầng biên (edge infrastructure). Máy chủ này hoạt động như trung tâm (hub) trong mô hình hub-and-spoke, cho phép trao đổi dữ liệu giữa control plane của Amazon EKS và các node Raspberry Pi, phục vụ cho các thao tác như kubectl exec, truy xuất log, và xử lý webhook.\nCác thiết bị Raspberry Pi chạy các thành phần node tiêu chuẩn của Kubernetes gồm kubelet, kube-proxy, và container runtime, cùng với công cụ dòng lệnh Amazon EKS Hybrid Nodes CLI (nodeadm). Các node này đăng ký với cụm EKS thông qua AWS Systems Manager, và hiển thị trong cụm như những worker node tiêu chuẩn, mặc dù được vận hành trên phần cứng do người dùng tự quản lý.\nCác node Raspberry Pi chủ động thiết lập kết nối với control plane của Amazon EKS thông qua Internet công cộng. Quá trình này bao gồm giao tiếp với API server để đăng ký node, cập nhật trạng thái pod, và gửi yêu cầu tài nguyên (resource requests). Phương thức public endpoint này giúp đơn giản hóa việc kết nối, đồng thời vẫn duy trì mức độ bảo mật cao nhờ xác thực qua AWS Identity and Access Management (IAM) và mã hóa TLS.\nBắt đầu thiết lập Để kết nối mạng giữa thiết bị Raspberry Pi và cụm EKS đang chạy trên đám mây, trước tiên chúng ta cấu hình một máy chủ WireGuard nhẹ trên một phiên bản EC2. Máy chủ này chỉ hoạt động như một cổng mạng (network gateway), vì vậy phiên bản EC2 t4g.nano tiết kiệm chi phí là đủ cho hầu hết các trường hợp sử dụng.\nSau khi máy chủ WireGuard được khởi động, chúng ta cài đặt client WireGuard trên Raspberry Pi để thiết lập kết nối liên tục (persistent connection), đồng thời cấu hình định tuyến phù hợp (routing) để cho phép trao đổi lưu lượng giữa Raspberry Pi và VPC mà cụm EKS đang sử dụng.\nTiếp theo, chúng ta thêm node lai (hybrid node) vào cụm EKS, cấu hình CNI (Container Network Interface), và cuối cùng cài đặt ứng dụng để hoàn thiện quá trình triển khai.\nYêu cầu:\nRaspberry Pi 5, chạy Ubuntu 24.10, bật SSH\nKết nối Ethernet có dây (khuyến nghị để đảm bảo ổn định)\nAWS Command Line Interface (AWS CLI)\nkubectl\nHelm\nBước 1: Tạo cụm EKS Bắt đầu bằng việc tạo một Amazon VPC trong Region AWS mà bạn chọn, với ít nhất một subnet công khai và một subnet riêng tư. Các subnet này sẽ chứa các worker node trên đám mây và các giao diện mạng cần thiết để giao tiếp với control plane. Khi thiết lập cụm EKS, hãy đảm bảo rằng các tham số mạng từ xa (remote networking parameters) được cấu hình để control plane có thể kết nối với các hybrid node và pod nằm ngoài VPC.\nĐể đơn giản hóa quá trình này, AWS cung cấp bộ Template Terraform trên AWS Samples GitHub repository. Các template này tự động hóa nhiều phần cấu hình mạng và Amazon EKS, chẳng hạn như kích hoạt hybrid networking và chuẩn bị các chính sách IAM và CNI cần thiết.\nNếu bạn mới làm quen với Amazon EKS Hybrid Nodes hoặc muốn tìm hiểu sâu hơn về quá trình cấu hình, hãy tham khảo tài liệu chính thức của AWS về việc kích hoạt cụm EKS cho Hybrid Nodes.\nBước 2: Thiết lập máy chủ VPN Amazon EKS Hybrid Nodes cần kết nối ổn định và mạng riêng giữa môi trường tại chỗ/biên và VPC của bạn. Điều này đòi hỏi thiết lập VPN hoặc giải pháp mạng riêng bảo mật tương tự. Có nhiều tùy chọn được AWS tài liệu hóa, chẳng hạn nhưAWS Site-to-Site VPN, AWS Direct Connect, hoặc kết nối VPN tự triển khai. Ở đây, chúng tôi sử dụng WireGuard, một phần mềm mã nguồn mở cho kết nối VPN nhanh và bảo mật.\n2.1 Cài đặt WireGuard Chúng tôi cài đặt WireGuard bằng cách triển khai máy chủ trên một EC2 instance trong tài khoản AWS của mình. Bạn có thể tham khảo bất kỳ hướng dẫn cài đặt WireGuard chuẩn nào để cấu hình máy chủ trên EC2, đảm bảo mở cổng UDP/51820 từ IP cục bộ tới security group của EC2. Bắt đầu bằng cách cài đặt WireGuard thông qua APT.\nsudo apt update \u0026amp;\u0026amp; sudo apt install -y wireguard sudo mkdir -p /etc/wireguard 2.2 Tạo cấu hình WireGuard\nTiếp theo, sử dụng trình soạn thảo mà bạn ưa thích để thêm cấu hình sau, thay thế các giá trị giữ chỗ (placeholders) bằng Public Key và Private Key của máy chủ WireGuard của bạn.\nsudo nano /etc/wireguard/wg0.conf Thêm cấu hình sau (thay thế các giá trị giữ chỗ):\n[Interface] PrivateKey = \u0026lt;client-private.key\u0026gt; Address = 10.200.0.2/24 [Peer] # Public key from AWS server (/etc/wireguard/public.key) PublicKey = \u0026lt;public.key\u0026gt; # Your EC2 instance\u0026#39;s public IP Endpoint = \u0026lt;ec2-public-ip\u0026gt;:51820 # WireGuard server network, AWS VPC CIDR \u0026amp; EKS Service CIDR AllowedIPs = 10.200.0.1/24,10.0.0.0/24,172.16.0.0/16 PersistentKeepalive = 25 Sau đó, kích hoạt dịch vụ WireGuard và xác minh rằng kết nối đã được thiết lập với máy chủ Amazon EC2.\nsudo systemctl enable wg-quick@wg0 sudo systemctl start wg-quick@wg0 sudo wg show Bạn sẽ thấy kết quả tương tự như sau:\ninterface: wg0 public key: zH6sK7s93lF4ZkPoe8L7TtyOe0e0zFqUYrqUJo1hXVA= private key: (hidden) listening port: 51820 peer: 8N3e1FzEJmGaJ8t6C2Zh1n3oA2uNfz8MZp4nCzHn3XA= endpoint: 52.14.123.45:51820 allowed ips: 10.0.0.0/16 latest handshake: 23 seconds ago transfer: 1.47 MiB received, 1.21 MiB sent persistent keepalive: every 25 seconds Như bước đầu tiên cho việc cấu hình mạng, cần bật IPv4 forwarding trên instance để nó có thể định tuyến các gói dữ liệu giữa các giao diện mạng:\necho \u0026#34;net.ipv4.ip_forward = 1\u0026#34; | sudo tee -a /etc/sysctl.conf sudo sysctl -p Tiếp theo, để cho phép EC2 instance chuyển tiếp lưu lượng giữa mạng WireGuard và VPC của bạn, hãy cấu hình iptables để thực hiện Network Address Translation (NAT) và cho phép chuyển tiếp gói dữ liệu (packet forwarding).\n# Enable masquerading for outgoing traffic via Wireguard interface iptables -t nat -A POSTROUTING -o wg0 -j MASQUERADE # Allow packets from VPC interface to be forwarded to Wireguard iptables -A FORWARD -i eth0 -o wg0 -j ACCEPT # Allow return traffic from Wireguard back into VPC for established connections iptables -A FORWARD -i wg0 -o eth0 -m state --state RELATED,ESTABLISHED -j ACCEPT Lệnh đầu tiên báo cho kernel ghi lại địa chỉ IP nguồn (source IP) của các gói dữ liệu đi qua giao diện WireGuard (wg0) thành địa chỉ IP của EC2 instance, điều này cần thiết để định tuyến lưu lượng trả về. Quy tắc thứ hai cho phép các gói từ giao diện VPC (eth0) được chuyển tiếp tới WireGuard. Quy tắc thứ ba cho phép lưu lượng trả về từ WireGuard quay lại VPC, nhưng chỉ áp dụng cho các kết nối đã được thiết lập trước đó.\nTiếp theo, để đảm bảo các quy tắc này được giữ nguyên sau khi khởi động lại (persist across reboots), hãy cài đặt và cấu hình gói iptables-persistent:\nsudo apt update sudo apt install iptables-persistent sudo netfilter-persistent save sudo systemctl enable netfilter-persistent Điều này lưu các quy tắc hiện tại vào/etc/iptables/rules.v4** và **/etc/iptables/rules.v6 và đảm bảo chúng được áp dụng tự động sau mỗi lần khởi động lại.\nỞ bước cuối cùng, hãy tắt tính năng kiểm tra source/destination (source/destination check) trên giao diện của instance. Theo mặc định, AWS bật kiểm tra source/destination để đảm bảo một instance chỉ xử lý lưu lượng được gửi đến hoặc đi từ chính nó. Tuy nhiên, vì instance của chúng ta đóng vai trò là gateway, định tuyến gói dữ liệu thay cho các thiết bị khác trong mạng, nên cần tắt giới hạn này.\nThêm Raspberry Pi vào cụm như một node từ xa** **Khi mạng đã được cấu hình và cụm EKS đã được tạo, bước tiếp theo là thêm node vào cụm để Kubernetes có thể bắt đầu lập lịch (scheduling) pod trên node này.\nTrước tiên, đảm bảo node có thể xác thực với cụm. Amazon EKS Hybrid Nodes xác thực với cụm EKS thông qua IAM, do đó cần gán IAM role cho các máy tại chỗ (on-premises). Điều này yêu cầu thiết lập cơ chế xác thực bằng Systems Manager hoặc IAM Roles Anywhere. Hướng dẫn trên GitHub sử dụng Systems Manager Hybrid Activations cho mục đích này. Bạn có thể theo hướng dẫn để tạo AmazonEKSHybridNodesRole bằng một trong hai tùy chọn. Sau đó, đăng ký node bằng nodeadm. Hãy theo dõi các hướng dẫn trong chỉ dẫn và chắc chắn chỉ định role mà bạn đã tạo ở bước trước.\nThiết lập Container Network Interface (CNI)\nSau khi cụm EKS và các hybrid node được tạo và cấu hình thành công, node của chúng ta vẫn hiển thị trạng thái Not Ready. Nguyên nhân là Container Network Interface (CNI) chưa được cài đặt. CNI là thành phần quan trọng chịu trách nhiệm thiết lập các giao diện mạng bên trong container, cấp phát địa chỉ IP, và cấu hình định tuyến để pod có thể giao tiếp liền mạch trong cụm và với mạng bên ngoài. Nếu không có CNI, các node Kubernetes không thể cung cấp kết nối mạng cần thiết cho pod, từ đó ngăn cản triển khai workload. Vì vậy, cần cài đặt CNI trước khi hybrid node sẵn sàng. Cilium là giải pháp mã nguồn mở, cloud-native để cung cấp, bảo mật và quan sát kết nối mạng giữa các workload, và được hỗ trợ chính thức cho Amazon EKS Hybrid Nodes\nBước 1: Cài đặt Cilium\nSau khi cài đặt Helm, chúng ta thêm Cilium Helm chart và cài đặt Cilium vào cụm EKS.\nTạo file cilium-values.yaml:\naffinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: eks.amazonaws.com/compute-type operator: In values: - hybrid ipam: mode: cluster-pool operator: clusterPoolIPv4MaskSize: 25 clusterPoolIPv4PodCIDRList: - 172.16.0.0/24 operator: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: eks.amazonaws.com/compute-type operator: In values: - hybrid unmanagedPodWatcher: restart: false envoy: enabled: false Sau đó chúng ta có thể cài đặt Cilium bằng Helm:\n[ec2-user@ip-10-0-6-175 terraform]$ helm repo add cilium https://helm.cilium.io/ \u0026gt; helm install cilium cilium/cilium \\ \u0026gt; --version 1.16.6 \\ \u0026gt; --namespace kube-system \\ \u0026gt; --values cilium-values.yaml NAME: cilium LAST DEPLOYED: Mon Apr 28 03:50:01 2025 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None Bạn đã cài đặt Cilium thành công, bây giờ hãy đợi cho đến khi cả hai pod đều sẵn sàng:\n[ec2-user@ip-10-0-6-175 terraform]$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system cilium-gzvhm 1/1 Running 0 4m50s kube-system cilium-operator-9c54b46b8-whgn9 1/1 Running 0 4m50s kube-system coredns-6d87fdb75-95wn2 1/1 Running 0 35s kube-system coredns-6d87fdb75-b2xf5 1/1 Running 0 35s kube-system kube-proxy-w48jx 1/1 Running 0 9m31s Bước 2: Xác minh các nút lai đang chạy\nChúng ta có thể kiểm tra xem tất cả các nút trong cụm EKS của mình có đang chạy thành công hay không. Chúng ta có thể kiểm tra trạng thái nút:\nkubectl get nodes Nút này hiện được đánh dấu là Sẵn sàng.\n[ec2-user@ip-10-0-6-175 terraform]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-6-175.ec2.internal Ready \u0026lt;none\u0026gt; 9m31s v1.30.9-eks-5d632ec Khi cụm của chúng ta hoạt động và mạng lưới container hoạt động như mong đợi, chúng ta sẽ thấy nút ở trạng thái Sẵn sàng trên Bảng thông tin tổng quan về nút Amazon EKS, như minh họa trong hình sau.\nTriển khai ứng dụng mẫu trên Amazon EKS Hybrid Nodes với tích hợp edge\nỨng dụng bao gồm hai deployment Kubernetes:\nUltrasonic: Đọc các giá trị đo từ cảm biến siêu âm và ghi vào DynamoDB. \\\nDashboard: Đọc dữ liệu từ DynamoDB và hiển thị trên giao diện tương tác (UI).\nChúng tôi sử dụng cảm biến siêu âm HC-SR04, thiết bị phát sóng âm và đo thời gian hồi âm để tính khoảng cách. Loại cảm biến này phổ biến trong các ngành sản xuất và ô tô, ví dụ:\nPhát hiện sự hiện diện hoặc vắng mặt của vật thể trên dây chuyền lắp ráp\nĐo mực chất lỏng trong các thùng chứa\nGiám sát tình trạng chỗ đỗ xe\nTrong một thiết lập nâng cao hơn, pipeline này có thể mở rộng để chạy mô hình nhận diện vật thể (object detection) tại chỗ và kích hoạt sự kiện, ví dụ gửi thông tin lên hàng đợi Amazon Simple Queue Service (Amazon SQS) dựa trên các điều kiện được phát hiện.\nTuy nhiên, trong demo này, chúng tôi ưu tiên tính rõ ràng và minh bạch. Node sẽ phát hiện khoảng cách của một vật thể đặt trước Raspberry Pi và đẩy giá trị này vào bảng DynamoDB mỗi 10 giây.\nBước 1: Yêu cầu phần cứng và thiết lập Cảm biến siêu âm HC-SR04:\nĐiện trở 1kΩ và 2kΩ (dùng trong mạch chia điện áp)\nDây nối (Jumper Wires)\nBreadboard\nChúng tôi sử dụng breadboard để nhanh chóng thử nghiệm mà không cần hàn. Breadboard giúp tối ưu hóa việc đi dây, hỗ trợ lặp nhanh, và đặt cảm biến HC-SR04 theo chiều đứng để tối ưu vị trí đo. Mỗi hàng trên breadboard chia sẻ điện liên tục, giúp đơn giản hóa kết nối.\nKết nối HC-SR04 với GPIO của Raspberry Pi\nKết nối chân 3.3V và GND của Raspberry Pi với đường nguồn (power rails) của breadboard.\nCắm cảm biến HC-SR04 vào breadboard. Sau đó kết nối:\nVCC → Breadboard + rail (dây đỏ)\nGND → Breadboard – rail (dây đen)\nTRIG → Raspberry Pi GPIO 4 (dây cam)\nECHO → Voltage divider → GPIO 17 (dây xanh)\nMạch chia điện áp sử dụng điện trở 1kΩ và 2kΩ mắc nối tiếp, giúp giảm tín hiệu 5V từ chân ECHO của cảm biến xuống khoảng 3.3V, an toàn cho GPIO của Raspberry Pi.\nSơ đồ minh họa được cung cấp để làm rõ cách bố trí này.\nBản đồ GPIO này sau này có thể được trừu tượng hóa và quản lý động qua Kubernetes ConfigMaps, giúp linh hoạt trong việc xử lý cấu hình phần cứng cho các deployment khác nhau. Chúng tôi sẽ trình bày chi tiết ở phần sau.\nBước 2: Triển khai bảng DynamoDB Chúng tôi lưu dữ liệu vào bảng DynamoDB có tên eks-timeseries, được tạo trong Region eu-west-1 . Bảng sử dụng schema như sau:\nPartition Key: yyyymmdd \\\nSort Key: hhmmss\nSchema này cho phép truy vấn theo thời gian một cách hiệu quả và phù hợp với các mô hình dữ liệu dạng time series, nơi dữ liệu được truy xuất theo ngày và sắp xếp theo dấu thời gian (timestamp).\nAWS CloudFormation template:\nResources: TimeSeriesTable: Type: AWS::DynamoDB::Table Properties: TableName: eks-timeseries AttributeDefinitions: - AttributeName: yyyymmdd AttributeType: S - AttributeName: hhmmss AttributeType: S KeySchema: - AttributeName: yyyymmdd KeyType: HASH - AttributeName: hhmmss KeyType: RANGE BillingMode: PAY_PER_REQUEST Tags: - Key: Environment Value: EdgeDemo Bước 3: Triển khai ứng dụng cảm biến Trong repository GitHub, có thư mục examples chứa dự án ultrasonic-demo. Thư mục này bao gồm:\nCác file manifest Kubernetes\nMã nguồn Python\nDockerfile để build image container\nBắt đầu bằng việc build Docker image từ thư mục ultrasonic-demo và đẩy lên container registry của bạn, ví dụ Amazon Elastic Container Registry (Amazon ECR).\nHãy chú ý phần ConfigMap trong manifest, vì nó định nghĩa các biến môi trường mà script Python cần để truy cập GPIO và DynamoDB, đồng thời cấu hình AWS CLI.\nĐể triển khai ứng dụng, chạy lệnh:\nkubectl apply -f manifest.yaml Sau khi triển khai, xác minh rằng pod ultrasonic-sensor đang chạy:\nkubectl get pods Tiếp theo, kiểm tra logs để giám sát dữ liệu từ cảm biến và các ghi chép vào DynamoDB:\nkubectl logs \u0026lt;pod-name\u0026gt; Bạn sẽ thấy các giá trị khoảng cách xuất hiện trong logs, và các kết quả tương tự cũng sẽ hiển thị trên bảng DynamoDB.\nBước 4: Triển khai frontend dashboard Để trực quan hóa dữ liệu từ cảm biến, chúng tôi xây dựng một frontend dashboard truy vấn dữ liệu trực tiếp từ DynamoDB và hiển thị dưới dạng biểu đồ cập nhật theo thời gian thực (live-updating chart).\nBất kỳ người dùng dữ liệu đã xác thực, kể cả các ứng dụng bên ngoài, đều có thể truy vấn DynamoDB trực tiếp.\nChúng tôi muốn tất cả ứng dụng đều được container hóa, do đó quyết định triển khai dashboard thông qua một deployment trong cụm.\nXem lại thư mục frontend trong repository để hiểu cấu trúc.\nBuild Docker image cho frontend và đẩy lên container registry, tương tự như cách đã làm với backend. Sau đó, cập nhật manifest Kubernetes được cung cấp.\nĐể triển khai ứng dụng, chạy lệnh:\nkubectl apply -f manifest.yaml Sau đó, bạn có thể thiết lập port-forwarding cho service trên máy local:\nkubectl port-forward svc/pi-dashboard 8080:80 Truy cập dashboard từ trình duyệt bằng cách mở http://localhost:8080.\nBạn sẽ thấy biểu đồ trực tiếp (live chart) cập nhật các giá trị khoảng cách theo thời gian thực, được truy xuất trực tiếp từ bảng DynamoDB.\nKết luận\nVậy là xong! Bạn vừa biến Raspberry Pi 5 thành một node của cụm Amazon EKS, hoạt động ngoài Amazon VPC, đọc dữ liệu thực từ cảm biến thông qua GPIO, và đẩy dữ liệu một cách an toàn lên đám mây bằng Amazon DynamoDB. Chúng tôi hy vọng ví dụ này với Raspberry Pi có thể làm minh họa thực tế cho cách kiến trúc Kubernetes lai (hybrid Kubernetes) kết nối các môi trường vật lý với đám mây, bất kể bạn đang làm việc với: Cảm biến trong nhà máy, Server tại cửa hàng bán lẻ, Engine xử lý inference trong bệnh viện hoặc sàn giao dịch. Đối với các tổ chức muốn hiện đại hóa hạ tầng phân tán, Amazon EKS Hybrid Nodes cung cấp một hướng đi thực tiễn. Bạn có thể build một lần và chạy trên đám mây, tại biên (edge), hoặc trên máy chủ bare metal của mình. Với sự linh hoạt và mạnh mẽ của phương pháp này, hiện tại là thời điểm lý tưởng để bắt đầu proof of concept và khám phá các khả năng cho tổ chức của bạn.\nMuốn tự thử nghiệm? Hãy tham khảo repository GitHub, clone ví dụ, và bắt đầu xây dựng. Ngoài ra, hãy xem hướng dẫn chính thức về Amazon EKS Hybrid Nodes, và liên hệ đội ngũ AWS của bạn nếu có thắc mắc khi bắt đầu.\n—\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\nVề các tác giả\nAlberto Crescini là Enterprise Solutions Architect tại AWS, hỗ trợ các công ty năng lượng và tiện ích ở Vương quốc Anh xây dựng hạ tầng cho quá trình chuyển đổi năng lượng. Anh hỗ trợ khách hàng trong các dự án như cân bằng lưới điện (grid balancing) và sản xuất năng lượng linh hoạt (flexible generation), đồng thời hướng dẫn họ hiện đại hóa hệ thống và nền tảng hyperscale thông qua lĩnh vực tập trung AWS Containers. Utkarsh Pundir là Containers Specialist Solutions Architect tại AWS, nơi anh hỗ trợ khách hàng xây dựng các giải pháp trên EKS. Các lĩnh vực trọng tâm của anh bao gồm kiến trúc lai (hybrid architecture) và triển khai workload Generative AI trên EKS như một phần của các sáng kiến go-to-market của AWS. Gladwin Neo là Containers Specialist Solutions Architect tại AWS, nơi anh hỗ trợ khách hàng di chuyển và hiện đại hóa các workload để triển khai trên Amazon Elastic Kubernetes Service (EKS) hoặc Amazon Elastic Container Service (ECS). "},{"uri":"https://datngo196.github.io/Internship_Report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs I have translated:\nBlog 1 - Chứng nhận ISO năm 2025 và CSA STAR hiện đã khả dụng cùng với hai dịch vụ bổ sung Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 2 - Tự động xoay vòng OIDC client secret với Application Load Balancer Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 3 - Sử dụng Raspberry Pi 5 như các nút lai (Hybrid Nodes) của Amazon EKS cho các khối tải (workload) tại biên (edge) Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\n"},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.3-event3/","title":"AWS AI/ML &amp; GenAI Workshop","tags":[],"description":"","content":"Summary Report: “AWS AI/ML \u0026amp; GenAI Workshop” Event Objectives Provide an overview of the AI/ML landscape and trends in Vietnam Guide hands-on practice on Amazon SageMaker for building end-to-end ML models Deep dive into Generative AI with Amazon Bedrock (Foundation Models, Agents, Guardrails) Equip attendees with Prompt Engineering skills and RAG (Retrieval-Augmented Generation) application building Speakers AWS Experts Team (Specific speaker names were not listed, but the session was led by technical experts) Key Highlights Welcome \u0026amp; Introduction Landscape Overview: Update on the panorama of Artificial Intelligence and Machine Learning (AI/ML) in the Vietnam market. Networking: Ice-breaker activity to create an open atmosphere for the workshop. AWS AI/ML Services Overview (SageMaker) End-to-end ML Platform: Understand the workflow on Amazon SageMaker from Data preparation, Labeling, to Training and Tuning models. MLOps Integration: Integrating Machine Learning Operations (MLOps) to automate deployment. SageMaker Studio Demo: Experience the interface and features of SageMaker Studio directly through a live walkthrough. Generative AI with Amazon Bedrock Foundation Models Selection: Comparison and guide on selecting suitable foundation models like Claude, Llama, Titan. Prompt Engineering: Optimization techniques: Chain-of-Thought, Few-shot learning. RAG Architecture: \u0026ldquo;Retrieval-Augmented Generation\u0026rdquo; architecture and how to integrate Knowledge Bases to increase AI accuracy. Advanced Features: Using Bedrock Agents for multi-step workflows and Guardrails for content safety. Live Demo: Building a complete GenAI Chatbot using Amazon Bedrock right in the class. Key Takeaways Platform Capabilities SageMaker is a powerful tool for traditional Machine Learning tasks, standardizing the process from data to model. Bedrock provides the fastest shortcut to access Generative AI via API without managing complex infrastructure. Strategic Implementation RAG \u0026amp; Agents are two key technologies that help GenAI applications solve complex business problems rather than just simple chatting. Guardrails are an indispensable component to ensure AI operates within safety frameworks and complies with corporate regulations. Applying to Work Implement MLOps: Apply standard processes on SageMaker to manage model lifecycles in current projects. Build RAG Systems: Experiment with integrating internal documents into Bedrock Knowledge Base to create information retrieval assistants. Optimize Prompts: Apply Chain-of-Thought techniques to improve the response quality of existing chatbots. Model Evaluation: Use the learned criteria to select the most suitable model (Claude vs Llama) in terms of cost and performance for each use case. Event Experience The workshop was a balanced combination of traditional Machine Learning and modern Generative AI, providing a solid foundational knowledge.\nHands-on \u0026amp; Demo The SageMaker Studio walkthrough helped me clearly visualize a professional working environment for Data Scientists. The Building a Chatbot with Bedrock demo was a highlight, proving that creating GenAI applications has become easier and faster than ever. Market Insight The introduction to the AI landscape in Vietnam helped me position my business within the general trend and identify potential opportunities. Some event photos Add your event photos here\nOverall, this event equipped me with a full \u0026ldquo;toolkit\u0026rdquo;: from SageMaker for Predictive models to Bedrock for Generative models, ready for upcoming AI projects.\n"},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Successfully build the Hybrid DNS network architecture to integrate the On-Premise DNS system (simulated by AWS Managed Microsoft Active Directory) with Amazon Route 53 DNS service. Successfully configure Inbound Endpoint, Outbound Endpoint, and Resolver Rules for bidirectional DNS query routing. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Preparation and Network Infrastructure Initialization: Review Route 53 Resolver features. Create an EC2 Key Pair (e.g., hr-dns-key) for secure Remote Desktop access. Initialize the CloudFormation Template (e.g., hr-dns-vpc-stack) to deploy the foundational high-availability and secure network infrastructure (VPC, Subnets, Gateways). 09/15/2025 09/15/2025 3 - Infrastructure Finalization \u0026amp; RDGW Connection: Monitor CloudFormation stack completion. Reconfigure the VPC Security Group (SG): remove unused ports (3391, 443) and retain ICMP and RDP (3389). Connect to the Remote Desktop Gateway (RDGW) Host by downloading the RDP file, uploading the Key Pair to decrypt, and retrieving the Administrator password. 09/16/2025 09/16/2025 4 - Route 53 Outbound Endpoint Setup: Create the Route 53 Outbound Endpoint to allow Route 53 Resolver to forward DNS queries externally (to AD). Select the correct VPC and corresponding Security Group. Configure automatic IP addresses in two different Availability Zones. Wait for the endpoint status to become operational. 09/17/2025 09/17/2025 5 - Resolver Rules and Inbound Endpoint Setup: Create a Resolver Rule (type Forward) to forward DNS queries for the specific domain (e.g., onprem.example.com) to the AWS Managed Microsoft AD DNS IP addresses. Create the Route 53 Inbound Endpoint to allow the On-Premise DNS system (AD) to query the Route 53 Resolver, configuring it within private subnets. 09/18/2025 09/18/2025 6 - Testing and Resource Cleanup: Test results using the nslookup onprem.example.com command on the RDGW host. Verify that the query is resolved via the VPC DNS Resolver IP (VPC CIDR + 2, e.g., 10.0.0.2). Clean up resources: Delete Inbound/Outbound Endpoints. Disassociate the VPC from the Resolver Rule before deleting the Rule. Delete AWS Managed Microsoft Active Directory, and finally, delete the CloudFormation Stacks. 09/19/2025 09/19/2025 Week 3 Achievements: Successfully implemented the Hybrid DNS architecture using Route 53 Resolver. Configured the Outbound Endpoint to allow Route 53 Resolver to forward DNS queries externally (to AWS Managed AD). Created the Inbound Endpoint enabling the On-Premise DNS system (AD) to query the Route 53 Resolver. Resolver Rules (Forward type) were successfully established to route queries for onprem.example.com to the AWS Managed Microsoft AD DNS IP addresses. Testing using nslookup on the RDGW host confirmed successful bidirectional DNS resolution. Verified that queries were resolved via the VPC DNS Resolver IP (e.g., 10.0.0.2, defined as VPC CIDR + 2). Completed comprehensive resource cleanup to prevent unexpected costs. "},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.4-event4/","title":"AWS DevOps &amp; Modern Operations","tags":[],"description":"","content":"Summary Report: “AWS DevOps \u0026amp; Modern Operations” Event Objectives Build a DevOps mindset and master key efficiency metrics (DORA metrics) Establish a complete CI/CD process using AWS Developer Tools Modernize infrastructure management with Infrastructure as Code (IaC) using CloudFormation and CDK Deploy containerized applications (Docker) on ECS, EKS, and App Runner Set up a comprehensive Observability system for distributed applications Speakers AWS Experts Team (Specialists in DevOps, Containers, and Observability) Key Highlights DevOps Mindset \u0026amp; CI/CD DORA Metrics: Understanding the importance of Deployment Frequency, Lead Time for Changes, MTTR, and Change Failure Rate. Git Strategies: Comparison of GitFlow and Trunk-based development strategies. Pipeline Automation: Demo of a full CI/CD pipeline from CodeCommit (source), CodeBuild (build/test) to CodeDeploy (deployment), orchestrated by CodePipeline. Deployment Strategies: Safe deployment techniques: Blue/Green, Canary, and Rolling updates. Infrastructure as Code (IaC) CloudFormation: Managing infrastructure via templates, concept of Stacks, and Drift detection. AWS CDK: Using familiar programming languages to define infrastructure, leveraging \u0026ldquo;Constructs\u0026rdquo; and reusable patterns. IaC Choice: Discussion on criteria for choosing between CloudFormation and CDK depending on the project. Container Services Spectrum of Compute: From image management (ECR) to orchestration options: ECS (simplified), EKS (standard Kubernetes), and App Runner (maximum simplification). Microservices Deployment: Comparison and demo of microservices deployment on different platforms. Monitoring \u0026amp; Observability Full-stack Observability: Combining CloudWatch (Metrics, Logs, Alarms) and X-Ray (Distributed Tracing) for a comprehensive view of system health. Best Practices: Setting up Monitoring Dashboards and effective On-call processes. Key Takeaways Automation First CI/CD is not just a toolset but a culture that minimizes human error and accelerates release velocity. IaC is a mandatory standard for modern infrastructure, ensuring consistency across Dev/Test/Prod environments. Operational Excellence Observability is more critical than simple Monitoring, especially in Microservices architectures for error tracing. Choosing the right deployment strategy (like Blue/Green) helps reduce Downtime to zero. Applying to Work Refactor Pipeline: Transition current manual build processes to AWS CodePipeline with automated testing steps. Adopt CDK: Start using AWS CDK to define infrastructure for new projects instead of manual Console operations. Containerization: Dockerize applications and pilot deployment on AWS App Runner for smaller services. Setup Tracing: Integrate AWS X-Ray into applications to monitor latency between microservices. Event Experience The full-day event was intense but the content was very cohesive, moving logically from Mindset to Tools and Operations.\nIntegrated Workflow The \u0026ldquo;Full CI/CD pipeline walkthrough\u0026rdquo; demo was impressive, showing the complete picture of how code moves from a developer\u0026rsquo;s machine to Production. Understanding the clear differences and specific use cases for ECS vs EKS gave me confidence in proposing solutions for the company. Practical Focus Lessons on Deployment strategies (Feature flags, Canary) were very practical for solving the team\u0026rsquo;s \u0026ldquo;deployment fear.\u0026rdquo; The Career roadmap at the end provided clear direction for developing DevOps skills. Some event photos Add your event photos here\nOverall, the workshop systematized the entire body of knowledge regarding modern operations, helping me understand the tight coupling between Code, Infrastructure, and Monitoring.\n"},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Deep dive into AWS Compute services (EC2) including scaling and storage options. Implement data protection strategies using AWS Backup. Configure Hybrid Cloud Storage using AWS Storage Gateway. Master Amazon S3 for static website hosting, content delivery (CloudFront), and data management (Versioning/Replication). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study EC2 core concepts including Instance types, AMIs, Key Pairs, differences between EBS and Instance Store, and practice launching an instance with User Data configuration 09/22/2025 09/22/2025 3 - Learn about EC2 Auto Scaling and Pricing options, then proceed to Lab 13 to deploy AWS Backup infrastructure, create a backup plan, and test the restoration process. 09/23/2025 09/23/2025 4 - Complete Lab 24 by setting up an EC2 for Storage Gateway, activating the Gateway, and creating File Shares. Start Lab 57 by creating an S3 bucket and loading initial data. 09/24/2025 09/24/2025 5 - Continue Lab 57 by enabling Static Website Hosting on S3, configuring public access settings, and setting up an Amazon CloudFront distribution to serve the website content. 09/25/2025 09/25/2025 6 - Finalize Lab 57 by exploring advanced features like Bucket Versioning, Lifecycle rules (moving objects), and Multi-Region Replication, followed by a complete resource cleanup. 09/26/2025 09/26/2025 Week 4 Achievements: Compute (EC2): Understood the differences between EBS and Instance Store. Learned how to use User Data to bootstrap instances. Configured EC2 Auto Scaling groups and understood pricing models. Data Protection: Successfully deployed AWS Backup to automate data backup and restoration procedures. Hybrid Storage: Created and configured AWS Storage Gateway to map file shares to cloud storage. Storage \u0026amp; CDN (S3 \u0026amp; CloudFront): Hosted a static website on Amazon S3. Integrated Amazon CloudFront for content delivery/caching. Implemented data management features: Versioning, Cross-Region Replication, and Lifecycle policies. "},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in five events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS First Cloud Journey Community Day\nDate \u0026amp; Time: 09:00, August 30, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Workshop\nDate \u0026amp; Time: 09:00, October 03, 2025\nLocation: Bitexco Tower (Floor 26 \u0026amp; 36), 02 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS AI/ML \u0026amp; GenAI Workshop\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: Bitexco Tower, 02 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS DevOps \u0026amp; Modern Operations\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: Bitexco Tower, 02 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: AWS Security Specialty Workshop\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: Bitexco Tower, 02 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nEvent 6 Event Name: Workshop: Data Science on AWS\nDate \u0026amp; Time: 09:30, October 16, 2025\nLocation: Hall Academic – FPT University, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.5-event5/","title":"AWS Security Specialty Workshop","tags":[],"description":"","content":"Summary Report: “AWS Security Specialty Workshop” Event Objectives Deeply understand the role of the Security Pillar within the Well-Architected Framework Master the 5 security pillars: IAM, Detection, Infrastructure, Data Protection, Incident Response Update on top security threats in the Vietnam cloud market Practice reviewing permissions (IAM) and building Incident Response (IR) Playbooks Speakers AWS Security Experts Team (Specialized in security architecture and compliance) Key Highlights Foundation \u0026amp; Identity (Pillar 1) Core Principles: Strictly applying Least Privilege, Zero Trust, and Defense in Depth principles. Modern IAM: Shifting from IAM Users (long-term credentials) to IAM Roles and AWS Identity Center (SSO) for centralized management. Access Control: Using Service Control Policies (SCPs) and Permission Boundaries to limit permission scopes in multi-account environments. Mini Demo: Practice Validating IAM Policies and simulating access to detect security flaws. Detection \u0026amp; Infrastructure (Pillar 2 \u0026amp; 3) Continuous Monitoring: Enabling CloudTrail (org-level), GuardDuty, and Security Hub for continuous monitoring. Logging Strategy: Logging at every layer: VPC Flow Logs (network), ALB logs (application), S3 logs (storage). Network Security: Network segmentation with VPC, combining Security Groups and NACLs. Edge protection with WAF, Shield, and Network Firewall. Data Protection \u0026amp; Incident Response (Pillar 4 \u0026amp; 5) Encryption: Encrypting data in-transit and at-rest on S3, EBS, RDS using KMS. Secrets Management: Eliminating hard-coded credentials by using Secrets Manager and Parameter Store with rotation mechanisms. IR Automation: Building Playbooks for common incidents (compromised keys, malware) and automating isolation processes using Lambda/Step Functions. Key Takeaways Zero Trust Mindset Identity is the new perimeter: In the Cloud environment, Identity is the most critical defense barrier, not IP addresses. Never trust by default; always verify and grant least privilege. Automation is Key Manual security cannot keep up with the speed of the Cloud. Detection-as-Code and Auto-remediation must be applied to minimize human risk. Applying to Work Review IAM: Audit all IAM Users, delete old keys, and switch to IAM Roles for applications. Enable GuardDuty: Activate GuardDuty across all regions/accounts to detect anomalous behaviors. Implement Secrets Manager: Replace config files containing DB passwords with API calls to Secrets Manager. Draft IR Playbook: Write an incident response procedure for \u0026ldquo;Compromised IAM Key\u0026rdquo; scenarios and rehearse with the team. Event Experience The workshop dove deep into technical details, comprehensively covering the security aspects that a Cloud Engineer needs to master.\nComprehensive Framework Structuring content by the 5 Pillars helped me systematize previously scattered security knowledge into a standard framework. The section on Top threats in Vietnam was very practical, helping identify specific risks within the local context. Practical Demos The demos on Access Analyzer and Validate IAM Policy were extremely useful, solving the daily headache of debugging permissions. The Incident Response section helped me understand that \u0026ldquo;detection\u0026rdquo; is only half the story; automated \u0026ldquo;response\u0026rdquo; is the goal. Some event photos Add your event photos here\nOverall, the event confirmed that security is not a blocker but an enabler that allows businesses to operate faster and more securely.\n"},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Master advanced Amazon S3 features (Storage Classes, Lifecycle, Access Points) and Glacier. Understand Hybrid Cloud solutions using AWS Storage Gateway and the Snow Family. Learn to migrate on-premises Virtual Machines to AWS (VM Import/Export). Deploy and manage Windows File Server file systems (FSx) with Multi-AZ configurations. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch Module 04 theory videos covering S3 Storage Classes, Access Points, Static Website Hosting, CORS, and the Snow Family, then start Lab 13 by creating S3 buckets and deploying backup infrastructure. 09/29/2025 09/29/2025 3 - Complete Lab 13 by setting up notifications and testing the restore process, then begin Lab 14 by preparing VMWare Workstation and exporting a virtual machine from on-premises environment.\n09/30/2025 09/30/2025 4 - Continue Lab 14 by uploading the VM to AWS, importing it as an AMI, and launching an instance; proceed to Lab 24 to initialize a Storage Gateway service. 10/01/2025 10/01/2025 5 - Finalize Lab 24 by creating File Shares and mounting them on a local machine, then start Lab 25 to create SSD and HDD Multi-AZ file systems (FSx for Windows File Server). 10/02/2025 10/02/2025 6 - Complete the setup of Multi-AZ file systems in Lab 25, review all storage concepts learned during the week, and perform a thorough cleanup of all created resources (S3, Gateways, File Systems). 10/03/2025 10/03/2025 Week 5 Achievements: Advanced Storage: Deepened understanding of S3 performance, security (CORS/ACL), and archival strategies with Glacier. Explored the AWS Snow Family for offline data transfer. Backup \u0026amp; Migration: Successfully configured AWS Backup notifications and tested data restoration. Performed a \u0026ldquo;Lift and Shift\u0026rdquo; migration by importing a VMWare virtual machine into AWS EC2. Hybrid \u0026amp; File Systems: Bridged on-premise and cloud storage using AWS Storage Gateway. Deployed highly available Windows File Systems (FSx) with Multi-AZ SSD/HDD configurations. "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.6-event6/","title":"Workshop: Data Science on AWS","tags":[],"description":"","content":"Summary Report: “Workshop: Data Science on AWS” Event Objectives Explore the complete journey of building a modern Data Science system from theory to practice. Understand the end-to-end Data Science Pipeline on AWS, from storage to processing and deployment. Gain hands-on experience with real-world datasets (IMDb) and practical models (Sentiment Analysis). Analyze the trade-offs between Cloud and On-premise infrastructures regarding cost and performance. Speakers Mr. Văn Hoàng Kha – Cloud Solutions Architect, AWS Community Builder Mr. Bạch Doãn Vương – Cloud DevOps Engineer, AWS Community Builder Key Highlights 1. Cloud in Data Science \u0026amp; Pipeline Overview Importance of Cloud: Discussed why modern data science relies on the cloud for scalability and integration, moving away from rigid on-premise constraints. The AWS Data Science Pipeline: Storage: Using Amazon S3 as the foundational data lake. ETL/Processing: Utilizing AWS Glue for serverless data integration. [cite_start]Modeling: Leveraging Amazon SageMaker as the central hub for building, training, and deploying models[cite: 212]. [cite_start]The session overviewed the broad AWS AI/ML stack, which encompasses AI services, ML services, and infrastructure[cite: 201]. 2. Practical Demos Demo 1: Data Processing with AWS Glue: Scenario: Processing and cleaning raw data from the IMDb dataset. Technique: Demonstrated how to handle feature engineering and data preparation efficiently. [cite_start]The workshop highlighted different approaches, ranging from low-code options like SageMaker Canvas [cite: 209] [cite_start]to code-first methods using Numpy/Pandas[cite: 210]. Demo 2: Sentiment Analysis with SageMaker: Scenario: Training and deploying a machine learning model to analyze text sentiment. [cite_start]Workflow: Showcased the \u0026ldquo;Train, Tune, Deploy\u0026rdquo; lifecycle within SageMaker Studio[cite: 212]. [cite_start]The session also touched upon \u0026ldquo;Bring Your Own Model\u0026rdquo; (BYOM) concepts, demonstrating flexibility with frameworks like TensorFlow and PyTorch[cite: 213]. 3. Strategic Discussions Cloud vs. On-Premise: A deep dive into cost optimization and performance metrics. The discussion highlighted how cloud elasticity allows for experimenting with heavy workloads without the massive upfront capital expenditure of on-premise hardware. Mini-Project Guidance: Introduction to a post-workshop project designed to reinforce the skills learned. Key Takeaways Technical Workflow Unified Pipeline: A robust data science workflow is not just about the code; it requires seamless integration between storage (S3), cleaning (Glue), and modeling (SageMaker). [cite_start]Tool Selection: Understanding when to use managed services (like Amazon Comprehend or Textract [cite: 202, 203]) versus building custom models on SageMaker is crucial for efficiency. Industry Application Real-world Context: The transition from academic theory to industry application lies in automation and scalability. Cost Awareness: Successful data projects must balance model accuracy with computational costs. Applying to Work Adopt AWS Glue: Migrate local ETL scripts to AWS Glue for automated, serverless data cleaning on larger datasets. SageMaker Deployment: Move experimental models from local Jupyter notebooks to SageMaker Studio to standardize the training and deployment process. Project Implementation: Execute the suggested mini-project to solidify understanding of the IMDb processing workflow. Event Experience The “Data Science on AWS” workshop was a bridge between university curriculum and enterprise reality.\nDirect Connection: It connected academic knowledge with the technologies used by top global enterprises. Practical Insight: Watching the IMDb dataset being cleaned and a Sentiment Analysis model being deployed live demystified the complexity of cloud-based AI. Expert Guidance: The interaction with AWS Community Builders provided deep insights into the \u0026ldquo;Cloud vs. On-premise\u0026rdquo; debate, helping me understand the strategic value of cloud migration beyond just technical features. Some event photos Add your event photos here\n"},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Master advanced administration of Amazon FSx for Windows File Server (Performance, Deduplication, Quotas). Deploy a global static website using Amazon S3 and Amazon CloudFront. Implement S3 data resiliency strategies (Versioning, Replication, Lifecycle). Understand the fundamentals of AWS Security, Identity, and Access Management (IAM \u0026amp; Cognito). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Deep dive into Lab 25 advanced features including creating file shares, testing and monitoring performance, enabling data deduplication, shadow copies, and managing user sessions and storage quotas. 10/06/2025 10/06/2025 3 - Finalize Lab 25 by practicing throughput and storage scaling, then delete the environment. Start Lab 57 by creating an S3 bucket, loading data, and enabling static website hosting features. 10/07/2025 10/07/2025 4 - Continue Lab 57 by configuring public access blocks and public objects, then set up and test an Amazon CloudFront distribution to serve the static website content with low latency. 10/08/2025 10/08/2025 5 - Complete Lab 57 with data management tasks: implementing Bucket Versioning, moving objects via Lifecycle rules, configuring Multi-Region Replication, and finally cleaning up all Lab resources. 10/09/2025 10/09/2025 6 - Begin Module 05 by studying the AWS Shared Responsibility Model, understanding the core concepts of AWS Identity and Access Management (IAM), and getting an overview of Amazon Cognito. 10/10/2025 10/10/2025 Week 6 Achievements: Advanced File Storage (FSx): Configured storage efficiency features like Data Deduplication and Shadow Copies. Managed user access controls through sessions and storage quotas. Performed scaling operations for throughput and storage capacity. Content Delivery \u0026amp; S3: Successfully hosted a static website on S3 and accelerated delivery using CloudFront CDN. Implemented data protection and lifecycle strategies (Versioning, Replication). Security Fundamentals: Grasped the Shared Responsibility Model between AWS and the customer. Understood the role of IAM for access control and Cognito for user identity management. "},{"uri":"https://datngo196.github.io/Internship_Report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd. from 08/09/2025 to 19/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment. I participated in FCJ Cloud Intern, through which I improved my skills in Cloud Computing architecture, Infrastructure as Code (AWS CDK), DevOps practices (CI/CD), Generative AI applications, and professional communication.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ☐ ✅ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Understand enterprise security structures using AWS Organizations and Identity Center. Implement security posture management with AWS Security Hub and encryption with KMS. Build automated incident response workflows using AWS Lambda and Slack integration. Master resource organization and management using Tagging strategies and Resource Groups. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch theory videos on AWS Organizations, Identity Center, KMS, and Security Hub. Complete Lab 18 by enabling Security Hub to assess security scores and then cleaning up resources. 10/13/2025 10/13/2025 3 - Begin Lab 22 to build an automated response system. Set up the network infrastructure (VPC, Security Groups), launch an EC2 instance, and configure an Incoming Webhook for Slack integration. 10/14/2025 10/14/2025 4 - Finalize Lab 22 by creating IAM Roles for Lambda, deploying functions to automatically stop/start instances, and verifying the automation results through Slack notifications before cleanup. 10/15/2025 10/15/2025 5 - Perform Lab 27 to focus on resource management. Practice launching EC2 instances with tags, managing tags via both Console and CLI, and filtering resources efficiently. 10/16/2025 10/16/2025 6 - Complete Lab 27 by creating Resource Groups based on tags and cleaning up. Start Lab 28 by learning how to create and configure a secure IAM User. 10/17/2025 10/17/2025 Week 7 Achievements: Security Governance: Gained insight into managing multi-account environments with AWS Organizations and Identity Center. Used AWS Security Hub to monitor compliance and security standards. Automation \u0026amp; Integration: Successfully built a serverless automation workflow using AWS Lambda to manage EC2 states. Integrated AWS services with third-party tools (Slack) for real-time monitoring. Resource Management: Applied advanced tagging strategies to organize cloud resources. Utilized Resource Groups and CLI commands for efficient bulk resource management. "},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.7-event7/","title":"Vietnam Cloud Day","tags":[],"description":"","content":"Summary Report: “Vietnam Cloud Day” Event Objectives Provide strategic insights for executive leadership on navigating the Generative AI revolution. Share best practices for building a unified, scalable data foundation on AWS. Introduce the AI-Driven Development Lifecycle (AI-DLC) and its impact on software implementation. Explore security fundamentals for Generative AI and the future of AI Agents. Speakers Eric Yeo – Country GM, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner – CEO, Techcombank Ms. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network Jaime Valles – VP \u0026amp; GM APJ, AWS Jeff Johnson, Vu Van (ELSA), Nguyen Hoa Binh (Nexttech), Dieter Botha (TymeX) – Panelists Kien Nguyen, Jun Kai Loke, Tamelly Lim, Binh Tran, Taiki Dang, Michael Armentano – AWS Specialists Key Highlights 1. Strategic Leadership \u0026amp; Vision Keynotes: Industry leaders from AWS, Techcombank, and U2U Network shared their vision for cloud and AI adoption in the region. Executive Panel: A discussion on \u0026ldquo;Navigating the GenAI Revolution,\u0026rdquo; focusing on fostering an innovation culture, aligning AI with business objectives, and managing organizational change during AI integration. 2. Data Foundation \u0026amp; Roadmap Unified Data Foundation: The session outlined how to build a robust infrastructure handling data ingestion, storage, processing, and governance—a critical prerequisite for advanced analytics and AI workloads. GenAI Roadmap: AWS presented its comprehensive vision and emerging trends to empower organizations to leverage GenAI for efficiency. 3. The Future of Software Development AI-Driven Development Lifecycle (AI-DLC): A transformative approach where AI is not just an assistant but a central collaborator. It integrates AI-powered execution with human oversight to drastically improve speed and innovation, moving beyond traditional methods. 4. Security \u0026amp; Advanced Automation Securing GenAI: Addressed security at three layers: infrastructure, models, and applications. Emphasized built-in measures like encryption, zero-trust architecture, and fine-grained access controls. AI Agents: The closing session highlighted a paradigm shift from basic automation to Intelligent Agents—partners that learn, adapt, and execute complex tasks autonomously. Key Takeaways Cultural Shift AI-DLC: Software development is evolving from \u0026ldquo;human-driven with AI assistance\u0026rdquo; to \u0026ldquo;AI-centric collaboration,\u0026rdquo; requiring a shift in how teams approach coding and testing. Agents vs. Automation: There is a distinct difference between static automation scripts and dynamic AI Agents that can make decisions and adapt to changing inputs. Technical Pillars Data First: You cannot have successful GenAI without a unified and governed data foundation. Security by Design: Security for GenAI must be continuous and layered, ensuring data confidentiality throughout the lifecycle. Applying to Work Assess Data Readiness: Review current AWS data infrastructure to ensure it meets the scalability and governance requirements for GenAI (per the Unified Data Foundation session). Explore AI Agents: Identify manual, complex operational tasks that could be offloaded to autonomous AI Agents rather than simple scripts. Adopt AI-DLC: Experiment with embedding AI tools more deeply into the dev lifecycle to act as collaborators rather than just code completers. Event Experience This summit provided a holistic view of the GenAI landscape, balancing high-level executive strategy with deep technical dives.\nStrategic Insight: The panel with leaders from ELSA, Nexttech, and TymeX offered valuable real-world perspectives on managing the cultural changes AI brings. Technical Depth: The afternoon tracks were particularly useful, specifically the deep dive into AI-DLC and Securing GenAI, which are immediate concerns for our technical roadmap. Some event photos Add your event photos here\n"},{"uri":"https://datngo196.github.io/Internship_Report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment: The environment at FCJ is highly professional and strongly encourages a \u0026ldquo;Builder\u0026rdquo; mindset. The workspace is always filled with positive energy, not only comfortable but also inspiring creativity. I particularly appreciate how everyone debates candidly and constructively, not afraid of friction to find the optimal solution. Additionally, interns like myself can sit at the same table and discuss directly with mentors without hierarchical barriers. The open space is a key factor that helps me maintain excitement every day when arriving at the office.\n2. Support from Mentor / Team Admin: My mentor acted not just as a technical guide but as a problem-solving mindset coach. Instead of simply giving \u0026ldquo;right or wrong\u0026rdquo; answers, he often asked, \u0026ldquo;Why did you choose this approach?\u0026rdquo; or \u0026ldquo;If the user base scales 10x, will this still work?\u0026rdquo; This coaching method helped me hone my critical thinking and be more meticulous with every line of code. Support from the admin team was very swift, especially in granting access to Cloud resources and paid tools necessary for work.\n3. Relevance of Work to Academic Major: Knowledge of Operating Systems, Computer Networks, or Data Structures learned at school provided a good foundation, but applying them to actual work here elevated them to a new level. I was exposed to real-world problems regarding Cloud Computing, Microservices, and DevOps – areas where school often stops at theory. However, this is exactly what I liked most because it forced me to apply foundational knowledge to real production problems, helping bridge the gap between a student and a professional engineer.\n4. Learning \u0026amp; Skill Development Opportunities: This internship felt like an intensive career boot camp. Regarding technical skills (Hard skills), I became more proficient in using Git flow, writing Infrastructure as Code (IaC), and debugging distributed systems. As for soft skills, I learned how to work within Agile/Scrum processes, how to write technical documentation that is easy for others to understand, and most importantly, communication skills – knowing how to ask the right questions to the right people at the right time to solve problems fastest.\n5. Company Culture \u0026amp; Team Spirit: The company culture values transparency and the spirit of \u0026ldquo;Knowledge Sharing.\u0026rdquo; When an incident occurs, instead of finding someone to blame, the whole team sits down to conduct a Root Cause Analysis to ensure the error doesn\u0026rsquo;t repeat. Teamwork is also exceptional; I never felt alone when facing difficulties because colleagues were always ready to support, share documentation, or pair-program to help me untangle issues. Tech Talk sessions are great opportunities to bond and learn from each other.\n6. Internship Policies / Benefits: Being supported with an AWS account for unrestricted practice (sandbox environment) is a fantastic benefit for technical roles. Additionally, policies supporting international certification exams are a huge motivation for me to strive harder.\nAdditional Questions Most satisfying aspect: I participated in real projects, deployed code to real environments, and saw the actual impact of the features I built. Area for improvement: The initial technical documentation onboarding process is a bit fragmented, taking quite some time to piece together. Would you recommend it: Definitely YES. This is an ideal environment for those passionate about Cloud and technology, who want to experience real-world working pressure. Suggestions \u0026amp; Expectations Suggestion: Should organize more cross-team \u0026ldquo;Code Review\u0026rdquo; sessions between different intern groups to learn from each other\u0026rsquo;s coding styles and thinking. Future intent: I strongly hope to have the opportunity to become a full-time employee (Fresher/Junior) to continue contributing to unfinished projects and further develop the skills I have just started to grasp. Other comments: Thank you to the company for creating a truly high-quality playground for students. "},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Master advanced IAM concepts: Cross-region access, Role Switching, and Attribute-Based Access Control (ABAC) using Tags. Implement restrictive security policies and test IAM User boundaries. Deploy comprehensive data security and auditing using KMS, CloudTrail, and Amazon Athena. Simulate real-world identity management scenarios with IAM Groups and Admin Roles. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Complete Lab 28 by creating IAM Policies and Roles, practicing Switch Roles to access resources in different regions (Tokyo/N. Virginia) based on Tag compliance. Perform Lab 30 to create restriction policies and test IAM user limits. 10/20/2025 10/20/2025 3 - Begin Lab 33 focused on audit and encryption. Create necessary IAM Policies, Roles, Groups, and Users, then initialize a Key Management Service (KMS) key and prepare an S3 bucket for secure data upload. 10/21/2025 10/21/2025 4 - Finalize Lab 33 by configuring CloudTrail to log API events, setting up Amazon Athena to query those logs, and testing the sharing of KMS-encrypted data on S3 before cleaning up. 10/22/2025 10/22/2025 5 - Start Lab 44 to reinforce IAM structural knowledge. Create IAM Groups and Users, and perform detailed permission checks to understand how policies affect user access rights. 10/23/2025 10/23/2025 6 - Complete Lab 44 by creating an Admin IAM Role and configuring the Switch Role mechanism for privilege elevation. Review all security concepts learned and perform a full resource cleanup. 10/24/2025 10/24/2025 Week 8 Achievements: Advanced Identity Management: Successfully implemented Switch Role mechanisms for secure cross-role access. Enforced access controls based on resource tags (ABAC) across different AWS regions. Managed user permissions effectively using Groups and Restriction Policies. Security \u0026amp; Compliance: Secured data at rest using AWS KMS encryption. Established an audit trail using AWS CloudTrail and analyzed logs using SQL queries in Amazon Athena. "},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.8-event8/","title":"Workshop: Data Science on AWS","tags":[],"description":"","content":"Summary Report: “Kick-off AWS First Cloud Journey Workforce OJT FALL 2025” Summary Report: “Workshop: Data Science on AWS” Event Objectives Officially launch the AWS First Cloud Journey (FCJ) Workforce OJT program for Fall 2025. Connect students with industry leaders from AWS, VNG, and G-Asia Pacific. Provide career orientation in Cloud Computing, DevOps, and GenAI. Share inspiring alumni stories and promote diversity with \u0026ldquo;She in Tech\u0026rdquo;. Speakers Mr. Nguyễn Trần Phước Bảo – Head of Enterprise Relations (School Representative) Mr. Nguyễn Gia Hưng – Head of Solutions Architect, AWS Vietnam Mr. Đỗ Huy Thắng – DevOps Lead, VNG Mr. Danh Hoàng Hiếu Nghị – GenAI Engineer, Renova Ms. Bùi Hồ Linh Nhi – AI Engineer, SoftwareOne Mr. Phạm Nguyễn Hải Anh – Cloud Engineer, G-Asia Pacific Mr. Nguyễn Đồng Thanh Hiệp – Principal Cloud Engineer, G-Asia Pacific Key Highlights 1. Opening \u0026amp; Vision Academic Partnership: Mr. Nguyễn Trần Phước Bảo opened the ceremony, emphasizing the strategic collaboration between the university and enterprises to bridge the gap between academic training and industry needs. Future Orientation: Mr. Nguyễn Gia Hưng (AWS) presented the vision of the \u0026ldquo;First Cloud Journey,\u0026rdquo; outlining how this OJT program serves as a launchpad for future cloud architects in Vietnam. 2. Career Pathways: DevOps \u0026amp; GenAI DevOps Reality: Mr. Đỗ Huy Thắng from VNG shared a realistic view of the DevOps career path, highlighting the essential skills required to survive and thrive in a major tech corporation. FCJ to GenAI: Alumni speakers Danh Hoàng Hiếu Nghị and Bùi Hồ Linh Nhi showcased their rapid evolution from FCJ interns to AI/GenAI Engineers, proving the program\u0026rsquo;s effectiveness. 3. Diversity \u0026amp; Daily Life She in Tech: Ms. Linh Nhi\u0026rsquo;s session highlighted the growing role of women in technology, encouraging female students to pursue technical roles in Cloud and AI. A Day in the Life: Speakers from G-Asia Pacific (Mr. Hải Anh \u0026amp; Mr. Hiệp) provided a transparent look at the daily responsibilities of a Cloud Engineer, from junior tasks to principal-level decision-making. Key Takeaways Career Roadmap Multiple Paths: The cloud industry offers diverse trajectories—from pure infrastructure (Cloud Engineer) to automation (DevOps) and cutting-edge innovation (GenAI). Foundation is Key: Success in specialized roles like GenAI starts with a strong foundation in Cloud Computing concepts gained during this OJT. Professional Mindset Adaptability: The transition from university to enterprise requires a shift in mindset—proactivity and continuous learning are non-negotiable. Community: Networking with mentors and alumni provides a crucial support system for career growth. Applying to Work Set OJT Goals: Define specific technical milestones (e.g., getting certified, mastering a specific AWS service) for the upcoming internship period based on speaker advice. Connect with Mentors: actively engage with the speakers and mentors introduced during the networking session. Explore GenAI: Dedicate time during the OJT to explore Generative AI services on AWS, as recommended by the alumni speakers. Event Experience Held at the Bitexco Financial Tower, the kick-off event set a professional and inspiring tone for the semester.\nAtmosphere: The energy was high, with a strong sense of community between the new cohort, alumni, and industry experts. Inspiration: Hearing former students share their success stories (\u0026ldquo;From FCJ to GenAI Engineer\u0026rdquo;) made the career goals feel tangible and achievable. Some event photos Add your event photos here\n"},{"uri":"https://datngo196.github.io/Internship_Report/4-eventparticipated/4.9-event9/","title":"CloudThinker: Agentic AI &amp; Orchestration on AWS","tags":[],"description":"","content":"Summary Report: “CloudThinker: Agentic AI \u0026amp; Orchestration on AWS” Event Objectives Deep dive into AWS Bedrock Agent Core and its capabilities. Explore real-world use cases for building Agentic Workflows. Understand advanced concepts like Agentic Orchestration and Context Optimization at a technical level (L300). Gain hands-on experience through the CloudThinker Hack workshop. Speakers Mr. Nguyen Gia Hung – Head of Solutions Architect, AWS Mr. Kien Nguyen – Solutions Architect, AWS Mr. Viet Pham – Founder \u0026amp; CEO Mr. Thang Ton – Co-founder \u0026amp; COO, CloudThinker Mr. Henry Bui – Head of Engineering, CloudThinker Mr. Kha Van – Workshop Facilitator Key Highlights 1. AWS Foundation Opening: Mr. Nguyen Gia Hung set the stage for the importance of Agentic AI in the current cloud landscape. Bedrock Agent Core: Mr. Kien Nguyen provided a technical overview of AWS Bedrock Agent Core, explaining how it simplifies the creation of agents that can plan and execute tasks by invoking APIs. 2. Practical Application \u0026amp; Use Cases Building Agentic Workflows: Mr. Viet Pham demonstrated a concrete use case, showing the end-to-end process of designing and deploying an agentic workflow on AWS. CloudThinker Introduction: Mr. Thang Ton introduced the CloudThinker ecosystem and their vision for AI-driven cloud solutions. 3. Deep Dive (Level 300) Agentic Orchestration \u0026amp; Context Optimization: This was the technical core of the morning. Mr. Henry Bui discussed advanced strategies for orchestrating multiple agents and optimizing context within Amazon Bedrock to ensure high accuracy and relevance in complex interactions. 4. Hands-on Experience CloudThinker Hack: Led by Mr. Kha Van, this 60-minute session allowed attendees to get their hands dirty, applying the morning\u0026rsquo;s concepts to build a prototype agent using the CloudThinker framework and AWS services. Key Takeaways Evolution of AI From Chat to Action: The industry is shifting from passive chatbots to active Agents that can perform complex orchestration and execute API calls. Context is Critical: As workflows get more complex, standard context windows aren\u0026rsquo;t enough. \u0026ldquo;Context Optimization\u0026rdquo; strategies are essential to keep costs down and accuracy up. Architecture Orchestration Patterns: Managing multiple agents require a robust orchestration layer to decide which agent handles which part of a user request. Applying to Work Prototype an Agent: Use AWS Bedrock Agent to build a simple internal tool that connects to a company API (e.g., checking leave balance or server status). Study Context Patterns: Research the context optimization techniques shared by Henry Bui to apply to our current RAG implementations. Participate in Hackathons: Encourage the team to join similar hands-on hacks to stay sharp on the latest AWS features. Event Experience This event was highly technical and focused.\nTechnical Depth: The L300 session on Orchestration was particularly valuable for understanding how to scale AI applications beyond simple demos. Interactive: The \u0026ldquo;CloudThinker Hack\u0026rdquo; provided immediate reinforcement of the theoretical knowledge, making it one of the most effective learning sessions. Some event photos Add your event photos here\n"},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Finalize IAM security best practices by comparing Access Keys vs. IAM Roles. Understand AWS Database services (RDS, Aurora, Redshift, ElastiCache) and general database concepts. Deploy a 2-tier architecture web application using Amazon EC2 and Amazon RDS. Perform database operations including backup, restoration, and connectivity via RDP. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Complete Lab 44 by implementing conditional switch roles (IP/Time limits). Proceed to Lab 48 to practice generating IAM Access Keys versus using IAM Roles for EC2, understanding why Roles are more secure. 10/27/2025 10/27/2025 3 - Study Module 06 theory videos covering fundamental database concepts (OLTP/OLAP), Amazon RDS \u0026amp; Aurora architectures, and overview of specialized DBs like Redshift and ElastiCache.\n10/28/2025 10/28/2025 4 - Start Lab 05 (Deploying Web App with RDS) by setting up the network foundation: creating a VPC, configuring EC2 and RDS Security Groups, defining DB Subnet Groups, and launching the EC2 instance. 10/29/2025 10/29/2025 5 - Continue Lab 05 by provisioning an Amazon RDS database instance, configuring the application on EC2 to connect to the database, and verifying the successful deployment of the web application. 10/30/2025 10/30/2025 6 - Finalize Lab 05 by performing database backup and restore operations, then clean up all resources. Begin Lab 43 by learning how to connect to Windows instances using an RDP Client. 10/31/2025 10/31/2025 Week 9 Achievements: IAM Security Mastery: Implemented advanced conditional access policies (Date/IP based). Demonstrated the security benefits of using IAM Roles over long-term Access Keys. Database Implementation: Grasped the differences between various AWS database offerings (Relational vs. Key-value vs. Warehousing). Successfully deployed a managed Relational Database (RDS) inside a VPC. Application Deployment: Connected a web application hosted on EC2 to a backend RDS instance. Performed critical database maintenance tasks like snapshots and restoration. "},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Perform heterogeneous database migrations (SQL Server/Oracle to Aurora MySQL) using AWS SCT and DMS. Troubleshoot complex migration scenarios involving schema conversion, memory pressure, and table errors. Build a Serverless Data Analytics pipeline using Amazon Kinesis, Glue, and Athena. Visualize business intelligence data using Amazon QuickSight. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Begin Lab 43 by connecting via EC2 Fleet Manager, configuring Source databases (SQL Server/Oracle), handling constraints, and preparing the Target Aurora MySQL environment. 11/03/2025 11/03/2025 3 - Continue Lab 43 by using the Schema Conversion Tool (SCT) to convert schemas, creating Migration Tasks and Endpoints, and launching a Serverless Migration process. 11/04/2025 11/04/2025 4 - Finalize Lab 43 by monitoring logs, troubleshooting specific test scenarios (Memory Pressure, Table Errors), validating the migrated data, and cleaning up migration resources. 11/05/2025 11/05/2025 5 - Start Module 07 (Lab 35) by setting up the ingestion layer: creating an S3 Bucket, configuring a Kinesis Data Firehose Delivery Stream, and generating sample data for analysis. 11/06/2025 11/06/2025 6 - Complete Lab 35 by configuring an AWS Glue Crawler to catalog data, performing SQL analysis with Amazon Athena, creating visualizations in QuickSight, and cleaning up resources. 11/07/2025 11/07/2025 Week 10 Achievements: Database Migration: Successfully migrated data from heterogeneous sources (SQL Server, Oracle) to AWS Aurora MySQL. Mastered the AWS Schema Conversion Tool (SCT) and Database Migration Service (DMS). Gained experience in troubleshooting migration failures (memory issues, mapping errors). Data Analytics Pipeline: Built a complete serverless data pipeline: Ingestion (Kinesis) -\u0026gt; Storage (S3) -\u0026gt; Catalog (Glue). Analyzed large datasets using SQL queries in Amazon Athena without managing servers. Business Intelligence: Connected Amazon QuickSight to the data source to create interactive visual reports. "},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Master NoSQL database design and operations using Amazon DynamoDB (Backup, Global Tables). Implement cost allocation strategies using Tagging and Cost Management tools. Utilize AWS developer tools (CloudShell, SDK) for programmatic resource management. Perform visual data preparation, profiling, and cleaning using AWS Glue DataBrew. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Deep dive into Amazon DynamoDB (Lab 39): Explore the console, perform backup/restore operations, and study advanced design patterns for building global serverless applications. 11/10/2025 11/10/2025 3 - Complete Lab 40 focusing on Cost Allocation: Build a database, populate data, apply tagging strategies to track usage, and query cost allocation tags to understand spending patterns. 11/11/2025 11/11/2025 4 - Perform Lab 60 to practice managing AWS resources using command-line interfaces: Amazon CloudShell and the AWS SDK, understanding the difference between Console and programmatic access. 11/12/2025 11/12/2025 5 - Begin Lab 70 for data preparation: Launch a Cloud9 environment, download and upload datasets to S3, set up AWS Glue DataBrew, and run data profiling to understand data quality. 11/13/2025 11/13/2025 6 - Finalize Lab 70 by cleaning and transforming data with DataBrew recipes. Proceed to Lab 72 to ingest, store, and catalog data into the AWS Glue Data Catalog, then clean up all resources. 11/14/2025 11/14/2025 Week 11 Achievements: Serverless Database (NoSQL): Gained proficiency in DynamoDB core concepts and event-driven architectures. Implemented backup strategies for NoSQL data. Cost \u0026amp; Management: Applied effective tagging strategies for granular cost tracking and reporting. Demonstrated ability to interact with AWS via CLI/SDK in CloudShell. Data Engineering: Utilized AWS Glue DataBrew to profile and clean raw data without writing code. Built a foundational data catalog for analytics workflows. "},{"uri":"https://datngo196.github.io/Internship_Report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Master advanced ETL (Extract, Transform, Load) processes using AWS Glue, DataBrew, and Amazon EMR. Implement real-time analytics with Kinesis and data warehousing with Amazon Redshift. Create professional, interactive Business Intelligence (BI) dashboards using Amazon QuickSight. Final Review: Consolidate knowledge from Modules 1-7 and finalize the end-of-term Worklog. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Perform advanced data transformation tasks in Lab 72 using various AWS Glue methods (Interactive Sessions, GUI, DataBrew) and process big data using Amazon EMR. 11/17/2025 11/17/2025 3 - Continue Lab 72 by running SQL queries with Amazon Athena, performing real-time analytics using Kinesis Data Analytics, and creating initial visualizations in QuickSight. 11/18/2025 11/18/2025 4 - Finalize Lab 72 by automating data serving via AWS Lambda and setting up a robust Data Warehouse using Amazon Redshift for complex query performance. 11/19/2025 11/19/2025 5 - Complete Lab 73 by building, refining, and publishing interactive Business Intelligence dashboards in Amazon QuickSight to visualize key insights.\n11/20/2025 11/20/2025 6 - General Review: Review key concepts from Modules 1 to 7, ensure all cloud resources are deleted to prevent costs, and finalize the complete Term Worklog for submission. 11/21/2025 11/21/2025 Week 12 Achievements: Advanced Data Engineering: Executed complex data transformations using Serverless (Glue) and Cluster-based (EMR) services. Built a Data Warehouse infrastructure using Amazon Redshift. Analytics \u0026amp; BI: Implemented real-time data stream processing with Kinesis. Designed and deployed interactive dashboards on Amazon QuickSight for decision-making support. Course Completion: Successfully reviewed the entire cloud journey (Compute, Storage, Database, Security, Analytics). Completed the final Worklog and cleaned up the AWS environment. "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.1-workshop-overview/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Introduction\u0026rdquo; date: 2025-09-09 weight : 1 chapter : false pre : \u0026quot; 5.1. \u0026quot;\nVPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.2-prerequiste/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Prerequiste\u0026rdquo; date: 2025-09-09 weight : 2 chapter : false pre : \u0026quot; 5.2. \u0026quot;\nIAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Create a gateway endpoint\u0026rdquo; date: 2025-09-09 weight : 1 chapter : false pre : \u0026quot; 5.3.1 \u0026quot;\nOpen the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Test the Gateway Endpoint\u0026rdquo; date: 2025-09-09 weight : 2 chapter : false pre : \u0026quot; 5.3.2 \u0026quot;\nCreate S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.3-s3-vpc/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Access S3 from VPC\u0026rdquo; date: 2025-09-09 weight : 3 chapter : false pre : \u0026quot; 5.3. \u0026quot;\nUsing Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Prepare the environment\u0026rdquo; date: 2025-09-09 weight : 1 chapter : false pre : \u0026quot; 5.4.1 \u0026quot;\nTo prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Create an S3 Interface endpoint\u0026rdquo; date: 2025-09-09 weight : 2 chapter : false pre : \u0026quot; 5.4.2 \u0026quot;\nIn this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Test the Interface Endpoint\u0026rdquo; date: 2025-09-09 weight : 3 chapter : false pre : \u0026quot; 5.4.3 \u0026quot;\nGet the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;On-premises DNS Simulation\u0026rdquo; date: 2025-09-09 weight : 4 chapter : false pre : \u0026quot; 5.4.4 \u0026quot;\nAWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.4-s3-onprem/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Access S3 from on-premises\u0026rdquo; date: 2025-09-09 weight : 4 chapter : false pre : \u0026quot; 5.4. \u0026quot;\nOverview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.5-policy/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;VPC Endpoint Policies\u0026rdquo; date: 2025-09-09 weight : 5 chapter : false pre : \u0026quot; 5.5. \u0026quot;\nWhen you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://datngo196.github.io/Internship_Report/5-workshop/5.6-cleanup/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Clean up\u0026rdquo; date: 2025-09-09 weight : 6 chapter : false pre : \u0026quot; 5.6. \u0026quot;\nCongratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://datngo196.github.io/Internship_Report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://datngo196.github.io/Internship_Report/tags/","title":"Tags","tags":[],"description":"","content":""}]